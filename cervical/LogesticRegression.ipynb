{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-07T20:18:17.567301Z",
     "start_time": "2024-11-07T20:18:17.565196Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:18:19.512503Z",
     "start_time": "2024-11-07T20:18:19.492001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('cleaned_data.csv')\n",
    "data.head()"
   ],
   "id": "a6d4083c584921f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Age  Number of sexual partners  First sexual intercourse  \\\n",
       "0   18                          6                         5   \n",
       "1   15                          0                         4   \n",
       "2   34                          0                         5   \n",
       "3   52                          7                         6   \n",
       "4   46                          5                        11   \n",
       "\n",
       "   Num of pregnancies  Smokes  Smokes (years)  Smokes (packs/year)  \\\n",
       "0                   1       0               0                    0   \n",
       "1                   1       0               0                    0   \n",
       "2                   1       0               0                    0   \n",
       "3                   6       1              23                   49   \n",
       "4                   6       0               0                    0   \n",
       "\n",
       "   Hormonal Contraceptives  Hormonal Contraceptives (years)  IUD  ...  \\\n",
       "0                        0                                0    0  ...   \n",
       "1                        0                                0    0  ...   \n",
       "2                        0                                0    0  ...   \n",
       "3                        1                               29    0  ...   \n",
       "4                        1                               20    0  ...   \n",
       "\n",
       "   STDs: Time since first diagnosis  STDs: Time since last diagnosis  \\\n",
       "0                                 0                                0   \n",
       "1                                 0                                0   \n",
       "2                                 0                                0   \n",
       "3                                 0                                0   \n",
       "4                                 0                                0   \n",
       "\n",
       "   Dx:Cancer  Dx:CIN  Dx:HPV  Dx  Hinselmann  Schiller  Citology  Biopsy  \n",
       "0          0       0       0   0           0         0         0       0  \n",
       "1          0       0       0   0           0         0         0       0  \n",
       "2          0       0       0   0           0         0         0       0  \n",
       "3          1       0       1   0           0         0         0       0  \n",
       "4          0       0       0   0           0         0         0       0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs: Time since first diagnosis</th>\n",
       "      <th>STDs: Time since last diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:18:25.312349Z",
     "start_time": "2024-11-07T20:18:25.308864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X=data.drop(['Biopsy'], axis=1)\n",
    "y=data['Biopsy']"
   ],
   "id": "c53bf14b08c8dc88",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:18:54.157381Z",
     "start_time": "2024-11-07T20:18:51.731507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN  # Import SMOTE and ADASYN\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_smote = []\n",
    "precision_list_smote = []\n",
    "recall_list_smote = []\n",
    "f1_list_smote = []\n",
    "entropy_list_smote = []\n",
    "confusion_matrices_smote = []\n",
    "\n",
    "accuracy_list_adasyn = []\n",
    "precision_list_adasyn = []\n",
    "recall_list_adasyn = []\n",
    "f1_list_adasyn = []\n",
    "entropy_list_adasyn = []\n",
    "confusion_matrices_adasyn = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model without SMOTE/ADASYN\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before SMOTE/ADASYN\n",
    "    y_pred_before = logreg_model.predict(X_test)\n",
    "    y_prob_before = logreg_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before SMOTE/ADASYN\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before SMOTE/ADASYN\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with SMOTE\n",
    "    logreg_model_smote = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_smote.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Predictions and probabilities after SMOTE\n",
    "    y_pred_smote = logreg_model_smote.predict(X_test)\n",
    "    y_prob_smote = logreg_model_smote.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after SMOTE\n",
    "    entropy_smote = calculate_entropy(y_prob_smote)\n",
    "    entropy_list_smote.append(entropy_smote)\n",
    "    \n",
    "    # Store metrics after SMOTE\n",
    "    accuracy_list_smote.append(accuracy_score(y_test, y_pred_smote))\n",
    "    precision_list_smote.append(precision_score(y_test, y_pred_smote))\n",
    "    recall_list_smote.append(recall_score(y_test, y_pred_smote))\n",
    "    f1_list_smote.append(f1_score(y_test, y_pred_smote))\n",
    "    confusion_matrices_smote.append(confusion_matrix(y_test, y_pred_smote))\n",
    "    \n",
    "    # Apply ADASYN to the training data\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with ADASYN\n",
    "    logreg_model_adasyn = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "    \n",
    "    # Predictions and probabilities after ADASYN\n",
    "    y_pred_adasyn = logreg_model_adasyn.predict(X_test)\n",
    "    y_prob_adasyn = logreg_model_adasyn.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after ADASYN\n",
    "    entropy_adasyn = calculate_entropy(y_prob_adasyn)\n",
    "    entropy_list_adasyn.append(entropy_adasyn)\n",
    "    \n",
    "    # Store metrics after ADASYN\n",
    "    accuracy_list_adasyn.append(accuracy_score(y_test, y_pred_adasyn))\n",
    "    precision_list_adasyn.append(precision_score(y_test, y_pred_adasyn))\n",
    "    recall_list_adasyn.append(recall_score(y_test, y_pred_adasyn))\n",
    "    f1_list_adasyn.append(f1_score(y_test, y_pred_adasyn))\n",
    "    confusion_matrices_adasyn.append(confusion_matrix(y_test, y_pred_adasyn))\n",
    "    \n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Oversampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_smote)} After SMOTE:\\n', confusion_matrices_smote[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_adasyn)} After ADASYN:\\n', confusion_matrices_adasyn[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before oversampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after SMOTE\n",
    "mean_accuracy_smote = np.mean(accuracy_list_smote)\n",
    "std_accuracy_smote = np.std(accuracy_list_smote)\n",
    "mean_precision_smote = np.mean(precision_list_smote)\n",
    "std_precision_smote = np.std(precision_list_smote)\n",
    "mean_recall_smote = np.mean(recall_list_smote)\n",
    "std_recall_smote = np.std(recall_list_smote)\n",
    "mean_f1_smote = np.mean(f1_list_smote)\n",
    "std_f1_smote = np.std(f1_list_smote)\n",
    "mean_entropy_smote = np.mean(entropy_list_smote)\n",
    "std_entropy_smote = np.std(entropy_list_smote)\n",
    "# Calculate mean and standard deviation of each metric after ADASYN\n",
    "mean_accuracy_adasyn = np.mean(accuracy_list_adasyn)\n",
    "std_accuracy_adasyn = np.std(accuracy_list_adasyn)\n",
    "mean_precision_adasyn = np.mean(precision_list_adasyn)\n",
    "std_precision_adasyn = np.std(precision_list_adasyn)\n",
    "mean_recall_adasyn = np.mean(recall_list_adasyn)\n",
    "std_recall_adasyn = np.std(recall_list_adasyn)\n",
    "mean_f1_adasyn = np.mean(f1_list_adasyn)\n",
    "std_f1_adasyn = np.std(f1_list_adasyn)\n",
    "mean_entropy_adasyn = np.mean(entropy_list_adasyn)\n",
    "std_entropy_adasyn = np.std(entropy_list_adasyn)\n",
    "\n",
    "# Calculate mean confusion matrix before and after SMOTE/ADASYN\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_smote = np.mean(confusion_matrices_smote, axis=0)\n",
    "mean_conf_matrix_adasyn = np.mean(confusion_matrices_adasyn, axis=0)\n",
    "\n",
    "# Print results before oversampling\n",
    "print('--- Before Oversampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after SMOTE\n",
    "print('--- After SMOTE ---')\n",
    "print('Mean Accuracy:', mean_accuracy_smote)\n",
    "print('Accuracy Std Dev:', std_accuracy_smote)\n",
    "print('Mean Precision:', mean_precision_smote)\n",
    "print('Precision Std Dev:', std_precision_smote)\n",
    "print('Mean Recall:', mean_recall_smote)\n",
    "print('Recall Std Dev:', std_recall_smote)\n",
    "print('Mean F1-score:', mean_f1_smote)\n",
    "print('F1-score Std Dev:', std_f1_smote)\n",
    "print('Mean Entropy:', mean_entropy_smote)\n",
    "print('Entropy Std Dev:', std_entropy_smote)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_smote)\n",
    "\n",
    "# Print results after ADASYN\n",
    "print('--- After ADASYN ---')\n",
    "print('Mean Accuracy:', mean_accuracy_adasyn)\n",
    "print('Accuracy Std Dev:', std_accuracy_adasyn)\n",
    "print('Mean Precision:', mean_precision_adasyn)\n",
    "print('Precision Std Dev:', std_precision_adasyn)\n",
    "print('Mean Recall:', mean_recall_adasyn)\n",
    "print('Recall Std Dev:', std_recall_adasyn)\n",
    "print('Mean F1-score:', mean_f1_adasyn)\n",
    "print('F1-score Std Dev:', std_f1_adasyn)\n",
    "print('Mean Entropy:', mean_entropy_adasyn)\n",
    "print('Entropy Std Dev:', std_entropy_adasyn)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_adasyn)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after ADASYN\n"
   ],
   "id": "161bd376eabcae19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After SMOTE:\n",
      " [[74  7]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After ADASYN:\n",
      " [[75  6]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 Before Oversampling:\n",
      " [[80  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After SMOTE:\n",
      " [[77  4]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 2 After ADASYN:\n",
      " [[78  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After SMOTE:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After ADASYN:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 4 After SMOTE:\n",
      " [[76  4]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 4 After ADASYN:\n",
      " [[75  5]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 Before Oversampling:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After SMOTE:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After ADASYN:\n",
      " [[76  4]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After SMOTE:\n",
      " [[76  4]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 6 After ADASYN:\n",
      " [[76  4]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After SMOTE:\n",
      " [[76  4]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After ADASYN:\n",
      " [[75  5]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 8 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After SMOTE:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After ADASYN:\n",
      " [[74  6]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 9 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After SMOTE:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After ADASYN:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 Before Oversampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After SMOTE:\n",
      " [[74  6]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After ADASYN:\n",
      " [[75  5]\n",
      " [ 0  5]]\n",
      "--- Before Oversampling ---\n",
      "Mean Accuracy: 0.9603693570451437\n",
      "Accuracy Std Dev: 0.012964473495922394\n",
      "Mean Precision: 0.7138095238095239\n",
      "Precision Std Dev: 0.1426577974228289\n",
      "Mean Recall: 0.6333333333333333\n",
      "Recall Std Dev: 0.1837873166945363\n",
      "Mean F1-score: 0.6576767676767676\n",
      "F1-score Std Dev: 0.1452098460780832\n",
      "Mean Entropy: 0.1181066652535481\n",
      "Entropy Std Dev: 0.014696915613700023\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.   3.5]]\n",
      "--- After SMOTE ---\n",
      "Mean Accuracy: 0.9429001367989056\n",
      "Accuracy Std Dev: 0.01760631145477418\n",
      "Mean Precision: 0.552478354978355\n",
      "Precision Std Dev: 0.12291733500194221\n",
      "Mean Recall: 0.82\n",
      "Recall Std Dev: 0.139204086785474\n",
      "Mean F1-score: 0.6496368827251181\n",
      "F1-score Std Dev: 0.09705594393506381\n",
      "Mean Entropy: 0.4026972398797515\n",
      "Entropy Std Dev: 0.026071682992730216\n",
      "Mean Confusion Matrix:\n",
      " [[76.4  3.9]\n",
      " [ 1.   4.5]]\n",
      "--- After ADASYN ---\n",
      "Mean Accuracy: 0.9370998632010943\n",
      "Accuracy Std Dev: 0.015681451341760546\n",
      "Mean Precision: 0.5123196248196249\n",
      "Precision Std Dev: 0.0750145678502953\n",
      "Mean Recall: 0.8033333333333335\n",
      "Recall Std Dev: 0.16428295373802143\n",
      "Mean F1-score: 0.6191844919786097\n",
      "F1-score Std Dev: 0.08426446601568151\n",
      "Mean Entropy: 0.3975742938429229\n",
      "Entropy Std Dev: 0.0272908104127139\n",
      "Mean Confusion Matrix:\n",
      " [[76.   4.3]\n",
      " [ 1.1  4.4]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:20:24.202619Z",
     "start_time": "2024-11-07T20:20:23.450547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "uncertainty_list = []  # List to store entropy for each fold\n",
    "confusion_matrices = []\n",
    "\n",
    "# Choose an oversampling method\n",
    "# oversampler = SMOTE(random_state=42)\n",
    "# oversampler = ADASYN(random_state=42)\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Function to calculate entropy\n",
    "def calculate_entropy(probabilities):\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities + 1e-9), axis=1)  # Add small value to avoid log(0)\n",
    "    return np.mean(entropy)\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Apply oversampling\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_pred_logreg = logreg_model.predict(X_test)\n",
    "    y_prob_logreg = logreg_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate and store entropy\n",
    "    entropy = calculate_entropy(y_prob_logreg)\n",
    "    uncertainty_list.append(entropy)\n",
    "    \n",
    "    # Store metrics\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred_logreg))\n",
    "    precision_list.append(precision_score(y_test, y_pred_logreg))\n",
    "    recall_list.append(recall_score(y_test, y_pred_logreg))\n",
    "    f1_list.append(f1_score(y_test, y_pred_logreg))\n",
    "    \n",
    "    # Store confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_logreg)\n",
    "    confusion_matrices.append(conf_matrix)\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices)}:\\n', conf_matrix)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "std_accuracy = np.std(accuracy_list)\n",
    "mean_precision = np.mean(precision_list)\n",
    "std_precision = np.std(precision_list)\n",
    "mean_recall = np.mean(recall_list)\n",
    "std_recall = np.std(recall_list)\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "\n",
    "# Calculate mean and standard deviation of entropy\n",
    "mean_entropy = np.mean(uncertainty_list)\n",
    "std_entropy = np.std(uncertainty_list)\n",
    "\n",
    "# Calculate mean confusion matrix\n",
    "mean_conf_matrix = np.mean(confusion_matrices, axis=0)\n",
    "\n",
    "# Print results\n",
    "print('Mean Accuracy:', mean_accuracy)\n",
    "print('Accuracy Std Dev:', std_accuracy)\n",
    "print('Mean Precision:', mean_precision)\n",
    "print('Precision Std Dev:', std_precision)\n",
    "print('Mean Recall:', mean_recall)\n",
    "print('Recall Std Dev:', std_recall)\n",
    "print('Mean F1-score:', mean_f1)\n",
    "print('F1-score Std Dev:', std_f1)\n",
    "print('Mean Entropy:', mean_entropy)\n",
    "print('Entropy Std Dev:', std_entropy)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix)\n"
   ],
   "id": "52aea4b01d47c8a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1:\n",
      " [[75  6]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3:\n",
      " [[78  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 4:\n",
      " [[76  4]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5:\n",
      " [[77  3]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7:\n",
      " [[76  4]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 9:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10:\n",
      " [[75  5]\n",
      " [ 0  5]]\n",
      "Mean Accuracy: 0.9545417236662107\n",
      "Accuracy Std Dev: 0.011012617783008348\n",
      "Mean Precision: 0.6087878787878788\n",
      "Precision Std Dev: 0.08137101514456908\n",
      "Mean Recall: 0.8866666666666667\n",
      "Recall Std Dev: 0.12927146286443544\n",
      "Mean F1-score: 0.71240675990676\n",
      "F1-score Std Dev: 0.06452716155065479\n",
      "Mean Entropy: 0.22420197465942104\n",
      "Entropy Std Dev: 0.024586544156110494\n",
      "Mean Confusion Matrix:\n",
      " [[77.   3.3]\n",
      " [ 0.6  4.9]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "UnderSampling\n",
   "id": "ea7150bcb1dd948d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:22:07.666870Z",
     "start_time": "2024-11-07T20:22:05.913138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import NearMiss, ClusterCentroids, TomekLinks\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_nearmiss = []\n",
    "precision_list_nearmiss = []\n",
    "recall_list_nearmiss = []\n",
    "f1_list_nearmiss = []\n",
    "entropy_list_nearmiss = []\n",
    "confusion_matrices_nearmiss = []\n",
    "\n",
    "accuracy_list_clustercentroids = []\n",
    "precision_list_clustercentroids = []\n",
    "recall_list_clustercentroids = []\n",
    "f1_list_clustercentroids = []\n",
    "entropy_list_clustercentroids = []\n",
    "confusion_matrices_clustercentroids = []\n",
    "\n",
    "accuracy_list_tomeklinks = []\n",
    "precision_list_tomeklinks = []\n",
    "recall_list_tomeklinks = []\n",
    "f1_list_tomeklinks = []\n",
    "entropy_list_tomeklinks = []\n",
    "confusion_matrices_tomeklinks = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model without undersampling\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before undersampling\n",
    "    y_pred_before = logreg_model.predict(X_test)\n",
    "    y_prob_before = logreg_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before undersampling\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before undersampling\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply NearMiss to the training data\n",
    "    nearmiss = NearMiss(version=1)\n",
    "    X_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with NearMiss\n",
    "    logreg_model_nearmiss = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_nearmiss.fit(X_train_nearmiss, y_train_nearmiss)\n",
    "    \n",
    "    # Predictions and probabilities after NearMiss\n",
    "    y_pred_nearmiss = logreg_model_nearmiss.predict(X_test)\n",
    "    y_prob_nearmiss = logreg_model_nearmiss.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after NearMiss\n",
    "    entropy_nearmiss = calculate_entropy(y_prob_nearmiss)\n",
    "    entropy_list_nearmiss.append(entropy_nearmiss)\n",
    "    \n",
    "    # Store metrics after NearMiss\n",
    "    accuracy_list_nearmiss.append(accuracy_score(y_test, y_pred_nearmiss))\n",
    "    precision_list_nearmiss.append(precision_score(y_test, y_pred_nearmiss))\n",
    "    recall_list_nearmiss.append(recall_score(y_test, y_pred_nearmiss))\n",
    "    f1_list_nearmiss.append(f1_score(y_test, y_pred_nearmiss))\n",
    "    confusion_matrices_nearmiss.append(confusion_matrix(y_test, y_pred_nearmiss))\n",
    "    \n",
    "    # Apply ClusterCentroids to the training data\n",
    "    clustercentroids = ClusterCentroids(random_state=42)\n",
    "    X_train_clustercentroids, y_train_clustercentroids = clustercentroids.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with ClusterCentroids\n",
    "    logreg_model_clustercentroids = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_clustercentroids.fit(X_train_clustercentroids, y_train_clustercentroids)\n",
    "    \n",
    "    # Predictions and probabilities after ClusterCentroids\n",
    "    y_pred_clustercentroids = logreg_model_clustercentroids.predict(X_test)\n",
    "    y_prob_clustercentroids = logreg_model_clustercentroids.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after ClusterCentroids\n",
    "    entropy_clustercentroids = calculate_entropy(y_prob_clustercentroids)\n",
    "    entropy_list_clustercentroids.append(entropy_clustercentroids)\n",
    "    \n",
    "    # Store metrics after ClusterCentroids\n",
    "    accuracy_list_clustercentroids.append(accuracy_score(y_test, y_pred_clustercentroids))\n",
    "    precision_list_clustercentroids.append(precision_score(y_test, y_pred_clustercentroids))\n",
    "    recall_list_clustercentroids.append(recall_score(y_test, y_pred_clustercentroids))\n",
    "    f1_list_clustercentroids.append(f1_score(y_test, y_pred_clustercentroids))\n",
    "    confusion_matrices_clustercentroids.append(confusion_matrix(y_test, y_pred_clustercentroids))\n",
    "    \n",
    "    # Apply TomekLinks to the training data\n",
    "    tomeklinks = TomekLinks()\n",
    "    X_train_tomeklinks, y_train_tomeklinks = tomeklinks.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with TomekLinks\n",
    "    logreg_model_tomeklinks = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_tomeklinks.fit(X_train_tomeklinks, y_train_tomeklinks)\n",
    "    \n",
    "    # Predictions and probabilities after TomekLinks\n",
    "    y_pred_tomeklinks = logreg_model_tomeklinks.predict(X_test)\n",
    "    y_prob_tomeklinks = logreg_model_tomeklinks.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after TomekLinks\n",
    "    entropy_tomeklinks = calculate_entropy(y_prob_tomeklinks)\n",
    "    entropy_list_tomeklinks.append(entropy_tomeklinks)\n",
    "    \n",
    "    # Store metrics after TomekLinks\n",
    "    accuracy_list_tomeklinks.append(accuracy_score(y_test, y_pred_tomeklinks))\n",
    "    precision_list_tomeklinks.append(precision_score(y_test, y_pred_tomeklinks))\n",
    "    recall_list_tomeklinks.append(recall_score(y_test, y_pred_tomeklinks))\n",
    "    f1_list_tomeklinks.append(f1_score(y_test, y_pred_tomeklinks))\n",
    "    confusion_matrices_tomeklinks.append(confusion_matrix(y_test, y_pred_tomeklinks))\n",
    "    \n",
    "    # Print confusion matrices for each method\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Undersampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_nearmiss)} After NearMiss:\\n', confusion_matrices_nearmiss[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_clustercentroids)} After ClusterCentroids:\\n', confusion_matrices_clustercentroids[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_tomeklinks)} After TomekLinks:\\n', confusion_matrices_tomeklinks[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before undersampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after NearMiss\n",
    "mean_accuracy_nearmiss = np.mean(accuracy_list_nearmiss)\n",
    "std_accuracy_nearmiss = np.std(accuracy_list_nearmiss)\n",
    "mean_precision_nearmiss = np.mean(precision_list_nearmiss)\n",
    "std_precision_nearmiss = np.std(precision_list_nearmiss)\n",
    "mean_recall_nearmiss = np.mean(recall_list_nearmiss)\n",
    "std_recall_nearmiss = np.std(recall_list_nearmiss)\n",
    "mean_f1_nearmiss = np.mean(f1_list_nearmiss)\n",
    "std_f1_nearmiss = np.std(f1_list_nearmiss)\n",
    "mean_entropy_nearmiss = np.mean(entropy_list_nearmiss)\n",
    "std_entropy_nearmiss = np.std(entropy_list_nearmiss)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after ClusterCentroids\n",
    "mean_accuracy_clustercentroids = np.mean(accuracy_list_clustercentroids)\n",
    "std_accuracy_clustercentroids = np.std(accuracy_list_clustercentroids)\n",
    "mean_precision_clustercentroids = np.mean(precision_list_clustercentroids)\n",
    "std_precision_clustercentroids = np.std(precision_list_clustercentroids)\n",
    "mean_recall_clustercentroids = np.mean(recall_list_clustercentroids)\n",
    "std_recall_clustercentroids = np.std(recall_list_clustercentroids)\n",
    "mean_f1_clustercentroids = np.mean(f1_list_clustercentroids)\n",
    "std_f1_clustercentroids = np.std(f1_list_clustercentroids)\n",
    "mean_entropy_clustercentroids = np.mean(entropy_list_clustercentroids)\n",
    "std_entropy_clustercentroids = np.std(entropy_list_clustercentroids)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after TomekLinks\n",
    "mean_accuracy_tomeklinks = np.mean(accuracy_list_tomeklinks)\n",
    "std_accuracy_tomeklinks = np.std(accuracy_list_tomeklinks)\n",
    "mean_precision_tomeklinks = np.mean(precision_list_tomeklinks)\n",
    "std_precision_tomeklinks = np.std(precision_list_tomeklinks)\n",
    "mean_recall_tomeklinks = np.mean(recall_list_tomeklinks)\n",
    "std_recall_tomeklinks = np.std(recall_list_tomeklinks)\n",
    "mean_f1_tomeklinks = np.mean(f1_list_tomeklinks)\n",
    "std_f1_tomeklinks = np.std(f1_list_tomeklinks)\n",
    "mean_entropy_tomeklinks = np.mean(entropy_list_tomeklinks)\n",
    "std_entropy_tomeklinks = np.std(entropy_list_tomeklinks)\n",
    "\n",
    "# Calculate mean confusion matrix before and after each method\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_nearmiss = np.mean(confusion_matrices_nearmiss, axis=0)\n",
    "mean_conf_matrix_clustercentroids = np.mean(confusion_matrices_clustercentroids, axis=0)\n",
    "mean_conf_matrix_tomeklinks = np.mean(confusion_matrices_tomeklinks, axis=0)\n",
    "\n",
    "# Print results before undersampling\n",
    "print('--- Before Undersampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after NearMiss\n",
    "print('--- After NearMiss ---')\n",
    "print('Mean Accuracy:', mean_accuracy_nearmiss)\n",
    "print('Accuracy Std Dev:', std_accuracy_nearmiss)\n",
    "print('Mean Precision:', mean_precision_nearmiss)\n",
    "print('Precision Std Dev:', std_precision_nearmiss)\n",
    "print('Mean Recall:', mean_recall_nearmiss)\n",
    "print('Recall Std Dev:', std_recall_nearmiss)\n",
    "print('Mean F1-score:', mean_f1_nearmiss)\n",
    "print('F1-score Std Dev:', std_f1_nearmiss)\n",
    "print('Mean Entropy:', mean_entropy_nearmiss)\n",
    "print('Entropy Std Dev:', std_entropy_nearmiss)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_nearmiss)\n",
    "\n",
    "# Print results after ClusterCentroids\n",
    "print('--- After ClusterCentroids ---')\n",
    "print('Mean Accuracy:', mean_accuracy_clustercentroids)\n",
    "print('Accuracy Std Dev:', std_accuracy_clustercentroids)\n",
    "print('Mean Precision:', mean_precision_clustercentroids)\n",
    "print('Precision Std Dev:', std_precision_clustercentroids)\n",
    "print('Mean Recall:', mean_recall_clustercentroids)\n",
    "print('Recall Std Dev:', std_recall_clustercentroids)\n",
    "print('Mean F1-score:', mean_f1_clustercentroids)\n",
    "print('F1-score Std Dev:', std_f1_clustercentroids)\n",
    "print('Mean Entropy:', mean_entropy_clustercentroids)\n",
    "print('Entropy Std Dev:', std_entropy_clustercentroids)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_clustercentroids)\n",
    "\n",
    "# Print results after TomekLinks\n",
    "print('--- After TomekLinks ---')\n",
    "print('Mean Accuracy:', mean_accuracy_tomeklinks)\n",
    "print('Accuracy Std Dev:', std_accuracy_tomeklinks)\n",
    "print('Mean Precision:', mean_precision_tomeklinks)\n",
    "print('Precision Std Dev:', std_precision_tomeklinks)\n",
    "print('Mean Recall:', mean_recall_tomeklinks)\n",
    "print('Recall Std Dev:', std_recall_tomeklinks)\n",
    "print('Mean F1-score:', mean_f1_tomeklinks)\n",
    "print('F1-score Std Dev:', std_f1_tomeklinks)\n",
    "print('Mean Entropy:', mean_entropy_tomeklinks)\n",
    "print('Entropy Std Dev:', std_entropy_tomeklinks)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_tomeklinks)\n"
   ],
   "id": "481fae0594864e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Undersampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After NearMiss:\n",
      " [[23 58]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After ClusterCentroids:\n",
      " [[67 14]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After TomekLinks:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 Before Undersampling:\n",
      " [[80  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After NearMiss:\n",
      " [[31 50]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 After ClusterCentroids:\n",
      " [[61 20]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 2 After TomekLinks:\n",
      " [[80  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 3 Before Undersampling:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After NearMiss:\n",
      " [[33 48]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 After ClusterCentroids:\n",
      " [[73  8]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After TomekLinks:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 4 After NearMiss:\n",
      " [[34 46]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 4 After ClusterCentroids:\n",
      " [[66 14]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 4 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 5 Before Undersampling:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 After ClusterCentroids:\n",
      " [[69 11]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After TomekLinks:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 6 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6 After ClusterCentroids:\n",
      " [[69 11]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After NearMiss:\n",
      " [[27 53]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 After ClusterCentroids:\n",
      " [[66 14]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 7 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 8 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After ClusterCentroids:\n",
      " [[59 21]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 9 Before Undersampling:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After NearMiss:\n",
      " [[30 50]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 9 After ClusterCentroids:\n",
      " [[75  5]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 Before Undersampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After NearMiss:\n",
      " [[23 57]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After ClusterCentroids:\n",
      " [[62 18]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After TomekLinks:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "--- Before Undersampling ---\n",
      "Mean Accuracy: 0.9603693570451437\n",
      "Accuracy Std Dev: 0.012964473495922394\n",
      "Mean Precision: 0.7138095238095239\n",
      "Precision Std Dev: 0.1426577974228289\n",
      "Mean Recall: 0.6333333333333333\n",
      "Recall Std Dev: 0.1837873166945363\n",
      "Mean F1-score: 0.6576767676767676\n",
      "F1-score Std Dev: 0.1452098460780832\n",
      "Mean Entropy: 0.1181066652535481\n",
      "Entropy Std Dev: 0.014696915613700023\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.   3.5]]\n",
      "--- After NearMiss ---\n",
      "Mean Accuracy: 0.4055129958960328\n",
      "Accuracy Std Dev: 0.04404933469840401\n",
      "Mean Precision: 0.09182917292817182\n",
      "Precision Std Dev: 0.013383718798270078\n",
      "Mean Recall: 0.93\n",
      "Recall Std Dev: 0.11396880664852506\n",
      "Mean F1-score: 0.1669103908256154\n",
      "F1-score Std Dev: 0.023212794514776317\n",
      "Mean Entropy: 0.17867048082804357\n",
      "Entropy Std Dev: 0.021577937509682437\n",
      "Mean Confusion Matrix:\n",
      " [[29.7 50.6]\n",
      " [ 0.4  5.1]]\n",
      "--- After ClusterCentroids ---\n",
      "Mean Accuracy: 0.8333926128590973\n",
      "Accuracy Std Dev: 0.054108781722101935\n",
      "Mean Precision: 0.2775377921087303\n",
      "Precision Std Dev: 0.077368129312292\n",
      "Mean Recall: 0.8666666666666668\n",
      "Recall Std Dev: 0.15420044674960504\n",
      "Mean F1-score: 0.4130328005328005\n",
      "F1-score Std Dev: 0.08931137461967233\n",
      "Mean Entropy: 0.49668329443373993\n",
      "Entropy Std Dev: 0.011337664684585223\n",
      "Mean Confusion Matrix:\n",
      " [[66.7 13.6]\n",
      " [ 0.7  4.8]]\n",
      "--- After TomekLinks ---\n",
      "Mean Accuracy: 0.9603693570451437\n",
      "Accuracy Std Dev: 0.012964473495922394\n",
      "Mean Precision: 0.7138095238095239\n",
      "Precision Std Dev: 0.1426577974228289\n",
      "Mean Recall: 0.6333333333333333\n",
      "Recall Std Dev: 0.1837873166945363\n",
      "Mean F1-score: 0.6576767676767676\n",
      "F1-score Std Dev: 0.1452098460780832\n",
      "Mean Entropy: 0.11978784996291461\n",
      "Entropy Std Dev: 0.01512358858931599\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.   3.5]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:23:19.975550Z",
     "start_time": "2024-11-07T20:23:17.975122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import NearMiss, ClusterCentroids, TomekLinks, RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_nearmiss = []\n",
    "precision_list_nearmiss = []\n",
    "recall_list_nearmiss = []\n",
    "f1_list_nearmiss = []\n",
    "entropy_list_nearmiss = []\n",
    "confusion_matrices_nearmiss = []\n",
    "\n",
    "accuracy_list_clustercentroids = []\n",
    "precision_list_clustercentroids = []\n",
    "recall_list_clustercentroids = []\n",
    "f1_list_clustercentroids = []\n",
    "entropy_list_clustercentroids = []\n",
    "confusion_matrices_clustercentroids = []\n",
    "\n",
    "accuracy_list_tomeklinks = []\n",
    "precision_list_tomeklinks = []\n",
    "recall_list_tomeklinks = []\n",
    "f1_list_tomeklinks = []\n",
    "entropy_list_tomeklinks = []\n",
    "confusion_matrices_tomeklinks = []\n",
    "\n",
    "accuracy_list_random = []\n",
    "precision_list_random = []\n",
    "recall_list_random = []\n",
    "f1_list_random = []\n",
    "entropy_list_random = []\n",
    "confusion_matrices_random = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model without undersampling\n",
    "    logreg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before undersampling\n",
    "    y_pred_before = logreg_model.predict(X_test)\n",
    "    y_prob_before = logreg_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before undersampling\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before undersampling\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply NearMiss to the training data\n",
    "    nearmiss = NearMiss(version=1)\n",
    "    X_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with NearMiss\n",
    "    logreg_model_nearmiss = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_nearmiss.fit(X_train_nearmiss, y_train_nearmiss)\n",
    "    \n",
    "    # Predictions and probabilities after NearMiss\n",
    "    y_pred_nearmiss = logreg_model_nearmiss.predict(X_test)\n",
    "    y_prob_nearmiss = logreg_model_nearmiss.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after NearMiss\n",
    "    entropy_nearmiss = calculate_entropy(y_prob_nearmiss)\n",
    "    entropy_list_nearmiss.append(entropy_nearmiss)\n",
    "    \n",
    "    # Store metrics after NearMiss\n",
    "    accuracy_list_nearmiss.append(accuracy_score(y_test, y_pred_nearmiss))\n",
    "    precision_list_nearmiss.append(precision_score(y_test, y_pred_nearmiss))\n",
    "    recall_list_nearmiss.append(recall_score(y_test, y_pred_nearmiss))\n",
    "    f1_list_nearmiss.append(f1_score(y_test, y_pred_nearmiss))\n",
    "    confusion_matrices_nearmiss.append(confusion_matrix(y_test, y_pred_nearmiss))\n",
    "    \n",
    "    # Apply ClusterCentroids to the training data\n",
    "    clustercentroids = ClusterCentroids(random_state=42)\n",
    "    X_train_clustercentroids, y_train_clustercentroids = clustercentroids.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with ClusterCentroids\n",
    "    logreg_model_clustercentroids = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_clustercentroids.fit(X_train_clustercentroids, y_train_clustercentroids)\n",
    "    \n",
    "    # Predictions and probabilities after ClusterCentroids\n",
    "    y_pred_clustercentroids = logreg_model_clustercentroids.predict(X_test)\n",
    "    y_prob_clustercentroids = logreg_model_clustercentroids.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after ClusterCentroids\n",
    "    entropy_clustercentroids = calculate_entropy(y_prob_clustercentroids)\n",
    "    entropy_list_clustercentroids.append(entropy_clustercentroids)\n",
    "    \n",
    "    # Store metrics after ClusterCentroids\n",
    "    accuracy_list_clustercentroids.append(accuracy_score(y_test, y_pred_clustercentroids))\n",
    "    precision_list_clustercentroids.append(precision_score(y_test, y_pred_clustercentroids))\n",
    "    recall_list_clustercentroids.append(recall_score(y_test, y_pred_clustercentroids))\n",
    "    f1_list_clustercentroids.append(f1_score(y_test, y_pred_clustercentroids))\n",
    "    confusion_matrices_clustercentroids.append(confusion_matrix(y_test, y_pred_clustercentroids))\n",
    "    \n",
    "    # Apply TomekLinks to the training data\n",
    "    tomeklinks = TomekLinks()\n",
    "    X_train_tomeklinks, y_train_tomeklinks = tomeklinks.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with TomekLinks\n",
    "    logreg_model_tomeklinks = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_tomeklinks.fit(X_train_tomeklinks, y_train_tomeklinks)\n",
    "    \n",
    "    # Predictions and probabilities after TomekLinks\n",
    "    y_pred_tomeklinks = logreg_model_tomeklinks.predict(X_test)\n",
    "    y_prob_tomeklinks = logreg_model_tomeklinks.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after TomekLinks\n",
    "    entropy_tomeklinks = calculate_entropy(y_prob_tomeklinks)\n",
    "    entropy_list_tomeklinks.append(entropy_tomeklinks)\n",
    "    \n",
    "    # Store metrics after TomekLinks\n",
    "    accuracy_list_tomeklinks.append(accuracy_score(y_test, y_pred_tomeklinks))\n",
    "    precision_list_tomeklinks.append(precision_score(y_test, y_pred_tomeklinks))\n",
    "    recall_list_tomeklinks.append(recall_score(y_test, y_pred_tomeklinks))\n",
    "    f1_list_tomeklinks.append(f1_score(y_test, y_pred_tomeklinks))\n",
    "    confusion_matrices_tomeklinks.append(confusion_matrix(y_test, y_pred_tomeklinks))\n",
    "    \n",
    "    # Apply RandomUnderSampler to the training data\n",
    "    random_under_sampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_random, y_train_random = random_under_sampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with RandomUnderSampler\n",
    "    logreg_model_random = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    logreg_model_random.fit(X_train_random, y_train_random)\n",
    "    \n",
    "    # Predictions and probabilities after RandomUnderSampler\n",
    "    y_pred_random = logreg_model_random.predict(X_test)\n",
    "    y_prob_random = logreg_model_random.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after RandomUnderSampler\n",
    "    entropy_random = calculate_entropy(y_prob_random)\n",
    "    entropy_list_random.append(entropy_random)\n",
    "    \n",
    "    # Store metrics after RandomUnderSampler\n",
    "    accuracy_list_random.append(accuracy_score(y_test, y_pred_random))\n",
    "    precision_list_random.append(precision_score(y_test, y_pred_random))\n",
    "    recall_list_random.append(recall_score(y_test, y_pred_random))\n",
    "    f1_list_random.append(f1_score(y_test, y_pred_random))\n",
    "    confusion_matrices_random.append(confusion_matrix(y_test, y_pred_random))\n",
    "    \n",
    "    # Print confusion matrices for each method\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Undersampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_nearmiss)} After NearMiss:\\n', confusion_matrices_nearmiss[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_clustercentroids)} After ClusterCentroids:\\n', confusion_matrices_clustercentroids[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_tomeklinks)} After TomekLinks:\\n', confusion_matrices_tomeklinks[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_random)} After RandomUnderSampler:\\n', confusion_matrices_random[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before undersampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after NearMiss\n",
    "mean_accuracy_nearmiss = np.mean(accuracy_list_nearmiss)\n",
    "std_accuracy_nearmiss = np.std(accuracy_list_nearmiss)\n",
    "mean_precision_nearmiss = np.mean(precision_list_nearmiss)\n",
    "std_precision_nearmiss = np.std(precision_list_nearmiss)\n",
    "mean_recall_nearmiss = np.mean(recall_list_nearmiss)\n",
    "std_recall_nearmiss = np.std(recall_list_nearmiss)\n",
    "mean_f1_nearmiss = np.mean(f1_list_nearmiss)\n",
    "std_f1_nearmiss = np.std(f1_list_nearmiss)\n",
    "mean_entropy_nearmiss = np.mean(entropy_list_nearmiss)\n",
    "std_entropy_nearmiss = np.std(entropy_list_nearmiss)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after ClusterCentroids\n",
    "mean_accuracy_clustercentroids = np.mean(accuracy_list_clustercentroids)\n",
    "std_accuracy_clustercentroids = np.std(accuracy_list_clustercentroids)\n",
    "mean_precision_clustercentroids = np.mean(precision_list_clustercentroids)\n",
    "std_precision_clustercentroids = np.std(precision_list_clustercentroids)\n",
    "mean_recall_clustercentroids = np.mean(recall_list_clustercentroids)\n",
    "std_recall_clustercentroids = np.std(recall_list_clustercentroids)\n",
    "mean_f1_clustercentroids = np.mean(f1_list_clustercentroids)\n",
    "std_f1_clustercentroids = np.std(f1_list_clustercentroids)\n",
    "mean_entropy_clustercentroids = np.mean(entropy_list_clustercentroids)\n",
    "std_entropy_clustercentroids = np.std(entropy_list_clustercentroids)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after TomekLinks\n",
    "mean_accuracy_tomeklinks = np.mean(accuracy_list_tomeklinks)\n",
    "std_accuracy_tomeklinks = np.std(accuracy_list_tomeklinks)\n",
    "mean_precision_tomeklinks = np.mean(precision_list_tomeklinks)\n",
    "std_precision_tomeklinks = np.std(precision_list_tomeklinks)\n",
    "mean_recall_tomeklinks = np.mean(recall_list_tomeklinks)\n",
    "std_recall_tomeklinks = np.std(recall_list_tomeklinks)\n",
    "mean_f1_tomeklinks = np.mean(f1_list_tomeklinks)\n",
    "std_f1_tomeklinks = np.std(f1_list_tomeklinks)\n",
    "mean_entropy_tomeklinks = np.mean(entropy_list_tomeklinks)\n",
    "std_entropy_tomeklinks = np.std(entropy_list_tomeklinks)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after RandomUnderSampler\n",
    "mean_accuracy_random = np.mean(accuracy_list_random)\n",
    "std_accuracy_random = np.std(accuracy_list_random)\n",
    "mean_precision_random = np.mean(precision_list_random)\n",
    "std_precision_random = np.std(precision_list_random)\n",
    "mean_recall_random = np.mean(recall_list_random)\n",
    "std_recall_random = np.std(recall_list_random)\n",
    "mean_f1_random = np.mean(f1_list_random)\n",
    "std_f1_random = np.std(f1_list_random)\n",
    "mean_entropy_random = np.mean(entropy_list_random)\n",
    "std_entropy_random = np.std(entropy_list_random)\n",
    "\n",
    "# Calculate mean confusion matrix before and after each method\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_nearmiss = np.mean(confusion_matrices_nearmiss, axis=0)\n",
    "mean_conf_matrix_clustercentroids = np.mean(confusion_matrices_clustercentroids, axis=0)\n",
    "mean_conf_matrix_tomeklinks = np.mean(confusion_matrices_tomeklinks, axis=0)\n",
    "mean_conf_matrix_random = np.mean(confusion_matrices_random, axis=0)\n",
    "\n",
    "# Print results before undersampling\n",
    "print('--- Before Undersampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after NearMiss\n",
    "print('--- After NearMiss ---')\n",
    "print('Mean Accuracy:', mean_accuracy_nearmiss)\n",
    "print('Accuracy Std Dev:', std_accuracy_nearmiss)\n",
    "print('Mean Precision:', mean_precision_nearmiss)\n",
    "print('Precision Std Dev:', std_precision_nearmiss)\n",
    "print('Mean Recall:', mean_recall_nearmiss)\n",
    "print('Recall Std Dev:', std_recall_nearmiss)\n",
    "print('Mean F1-score:', mean_f1_nearmiss)\n",
    "print('F1-score Std Dev:', std_f1_nearmiss)\n",
    "print('Mean Entropy:', mean_entropy_nearmiss)\n",
    "print('Entropy Std Dev:', std_entropy_nearmiss)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_nearmiss)\n",
    "\n",
    "# Print results after ClusterCentroids\n",
    "print('--- After ClusterCentroids ---')\n",
    "print('Mean Accuracy:', mean_accuracy_clustercentroids)\n",
    "print('Accuracy Std Dev:', std_accuracy_clustercentroids)\n",
    "print('Mean Precision:', mean_precision_clustercentroids)\n",
    "print('Precision Std Dev:', std_precision_clustercentroids)\n",
    "print('Mean Recall:', mean_recall_clustercentroids)\n",
    "print('Recall Std Dev:', std_recall_clustercentroids)\n",
    "print('Mean F1-score:', mean_f1_clustercentroids)\n",
    "print('F1-score Std Dev:', std_f1_clustercentroids)\n",
    "print('Mean Entropy:', mean_entropy_clustercentroids)\n",
    "print('Entropy Std Dev:', std_entropy_clustercentroids)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_clustercentroids)\n",
    "\n",
    "# Print results after TomekLinks\n",
    "print('--- After TomekLinks ---')\n",
    "print('Mean Accuracy:', mean_accuracy_tomeklinks)\n",
    "print('Accuracy Std Dev:', std_accuracy_tomeklinks)\n",
    "print('Mean Precision:', mean_precision_tomeklinks)\n",
    "print('Precision Std Dev:', std_precision_tomeklinks)\n",
    "print('Mean Recall:', mean_recall_tomeklinks)\n",
    "print('Recall Std Dev:', std_recall_tomeklinks)\n",
    "print('Mean F1-score:', mean_f1_tomeklinks)\n",
    "print('F1-score Std Dev:', std_f1_tomeklinks)\n",
    "print('Mean Entropy:', mean_entropy_tomeklinks)\n",
    "print('Entropy Std Dev:', std_entropy_tomeklinks)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_tomeklinks)\n",
    "\n",
    "# Print results after RandomUnderSampler\n",
    "print('--- After RandomUnderSampler ---')\n",
    "print('Mean Accuracy:', mean_accuracy_random)\n",
    "print('Accuracy Std Dev:', std_accuracy_random)\n",
    "print('Mean Precision:', mean_precision_random)\n",
    "print('Precision Std Dev:', std_precision_random)\n",
    "print('Mean Recall:', mean_recall_random)\n",
    "print('Recall Std Dev:', std_recall_random)\n",
    "print('Mean F1-score:', mean_f1_random)\n",
    "print('F1-score Std Dev:', std_f1_random)\n",
    "print('Mean Entropy:', mean_entropy_random)\n",
    "print('Entropy Std Dev:', std_entropy_random)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_random)\n"
   ],
   "id": "8bf1418d9ee31ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Undersampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After NearMiss:\n",
      " [[23 58]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After ClusterCentroids:\n",
      " [[67 14]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After TomekLinks:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After RandomUnderSampler:\n",
      " [[75  6]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 Before Undersampling:\n",
      " [[80  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After NearMiss:\n",
      " [[31 50]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 After ClusterCentroids:\n",
      " [[61 20]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 2 After TomekLinks:\n",
      " [[80  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After RandomUnderSampler:\n",
      " [[79  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 Before Undersampling:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After NearMiss:\n",
      " [[33 48]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 After ClusterCentroids:\n",
      " [[73  8]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After TomekLinks:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After RandomUnderSampler:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 4 After NearMiss:\n",
      " [[34 46]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 4 After ClusterCentroids:\n",
      " [[66 14]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 4 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 4 After RandomUnderSampler:\n",
      " [[72  8]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 Before Undersampling:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 After ClusterCentroids:\n",
      " [[69 11]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After TomekLinks:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After RandomUnderSampler:\n",
      " [[76  4]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6 After ClusterCentroids:\n",
      " [[69 11]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After RandomUnderSampler:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After NearMiss:\n",
      " [[27 53]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 After ClusterCentroids:\n",
      " [[66 14]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 7 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After RandomUnderSampler:\n",
      " [[73  7]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After NearMiss:\n",
      " [[32 48]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After ClusterCentroids:\n",
      " [[59 21]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After RandomUnderSampler:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 9 Before Undersampling:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After NearMiss:\n",
      " [[30 50]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 9 After ClusterCentroids:\n",
      " [[75  5]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After RandomUnderSampler:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 Before Undersampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After NearMiss:\n",
      " [[23 57]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After ClusterCentroids:\n",
      " [[64 16]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After TomekLinks:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After RandomUnderSampler:\n",
      " [[71  9]\n",
      " [ 1  4]]\n",
      "--- Before Undersampling ---\n",
      "Mean Accuracy: 0.9603693570451437\n",
      "Accuracy Std Dev: 0.012964473495922394\n",
      "Mean Precision: 0.7138095238095239\n",
      "Precision Std Dev: 0.1426577974228289\n",
      "Mean Recall: 0.6333333333333333\n",
      "Recall Std Dev: 0.1837873166945363\n",
      "Mean F1-score: 0.6576767676767676\n",
      "F1-score Std Dev: 0.1452098460780832\n",
      "Mean Entropy: 0.1181066652535481\n",
      "Entropy Std Dev: 0.014696915613700023\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.   3.5]]\n",
      "--- After NearMiss ---\n",
      "Mean Accuracy: 0.4055129958960328\n",
      "Accuracy Std Dev: 0.04404933469840401\n",
      "Mean Precision: 0.09182917292817182\n",
      "Precision Std Dev: 0.013383718798270078\n",
      "Mean Recall: 0.93\n",
      "Recall Std Dev: 0.11396880664852506\n",
      "Mean F1-score: 0.1669103908256154\n",
      "F1-score Std Dev: 0.023212794514776317\n",
      "Mean Entropy: 0.17867048082804357\n",
      "Entropy Std Dev: 0.021577937509682437\n",
      "Mean Confusion Matrix:\n",
      " [[29.7 50.6]\n",
      " [ 0.4  5.1]]\n",
      "--- After ClusterCentroids ---\n",
      "Mean Accuracy: 0.8357455540355678\n",
      "Accuracy Std Dev: 0.052584049099493275\n",
      "Mean Precision: 0.2796081854834715\n",
      "Precision Std Dev: 0.07599573943498956\n",
      "Mean Recall: 0.8666666666666668\n",
      "Recall Std Dev: 0.15420044674960504\n",
      "Mean F1-score: 0.41578005328005335\n",
      "F1-score Std Dev: 0.08796226812859817\n",
      "Mean Entropy: 0.49597304339705106\n",
      "Entropy Std Dev: 0.011528901018276434\n",
      "Mean Confusion Matrix:\n",
      " [[66.9 13.4]\n",
      " [ 0.7  4.8]]\n",
      "--- After TomekLinks ---\n",
      "Mean Accuracy: 0.9603693570451437\n",
      "Accuracy Std Dev: 0.012964473495922394\n",
      "Mean Precision: 0.7138095238095239\n",
      "Precision Std Dev: 0.1426577974228289\n",
      "Mean Recall: 0.6333333333333333\n",
      "Recall Std Dev: 0.1837873166945363\n",
      "Mean F1-score: 0.6576767676767676\n",
      "F1-score Std Dev: 0.1452098460780832\n",
      "Mean Entropy: 0.11978784996291461\n",
      "Entropy Std Dev: 0.01512358858931599\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.   3.5]]\n",
      "--- After RandomUnderSampler ---\n",
      "Mean Accuracy: 0.9370177838577292\n",
      "Accuracy Std Dev: 0.026861202747308518\n",
      "Mean Precision: 0.5404331779331779\n",
      "Precision Std Dev: 0.13855815589884485\n",
      "Mean Recall: 0.85\n",
      "Recall Std Dev: 0.11761519176251568\n",
      "Mean F1-score: 0.6454703045492519\n",
      "F1-score Std Dev: 0.10035556106425562\n",
      "Mean Entropy: 0.4275727674667597\n",
      "Entropy Std Dev: 0.02567984465343371\n",
      "Mean Confusion Matrix:\n",
      " [[75.7  4.6]\n",
      " [ 0.8  4.7]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Smote+tomek",
   "id": "e3a2f590544ef80d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:27:41.940758Z",
     "start_time": "2024-11-07T20:27:41.238963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.combine import SMOTETomek  # Import SMOTE + Tomek Links\n",
    "from sklearn.linear_model import LogisticRegression  # Import Logistic Regression\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_smote_tomek = []\n",
    "precision_list_smote_tomek = []\n",
    "recall_list_smote_tomek = []\n",
    "f1_list_smote_tomek = []\n",
    "entropy_list_smote_tomek = []\n",
    "confusion_matrices_smote_tomek = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model without SMOTE + Tomek Links\n",
    "    lr_model = LogisticRegression(random_state=42)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before SMOTE + Tomek Links\n",
    "    y_pred_before = lr_model.predict(X_test)\n",
    "    y_prob_before = lr_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before SMOTE + Tomek Links\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before SMOTE + Tomek Links\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before, average='weighted'))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before, average='weighted'))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before, average='weighted'))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply SMOTE + Tomek Links to the training data\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_smote_tomek, y_train_smote_tomek = smote_tomek.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model with SMOTE + Tomek Links\n",
    "    lr_model_smote_tomek = LogisticRegression(random_state=42)\n",
    "    lr_model_smote_tomek.fit(X_train_smote_tomek, y_train_smote_tomek)\n",
    "    \n",
    "    # Predictions and probabilities after SMOTE + Tomek Links\n",
    "    y_pred_smote_tomek = lr_model_smote_tomek.predict(X_test)\n",
    "    y_prob_smote_tomek = lr_model_smote_tomek.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after SMOTE + Tomek Links\n",
    "    entropy_smote_tomek = calculate_entropy(y_prob_smote_tomek)\n",
    "    entropy_list_smote_tomek.append(entropy_smote_tomek)\n",
    "    \n",
    "    # Store metrics after SMOTE + Tomek Links\n",
    "    accuracy_list_smote_tomek.append(accuracy_score(y_test, y_pred_smote_tomek))\n",
    "    precision_list_smote_tomek.append(precision_score(y_test, y_pred_smote_tomek))\n",
    "    recall_list_smote_tomek.append(recall_score(y_test, y_pred_smote_tomek))\n",
    "    f1_list_smote_tomek.append(f1_score(y_test, y_pred_smote_tomek))\n",
    "    confusion_matrices_smote_tomek.append(confusion_matrix(y_test, y_pred_smote_tomek))\n",
    "    \n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Oversampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_smote_tomek)} After SMOTE + Tomek Links:\\n', confusion_matrices_smote_tomek[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before oversampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after SMOTE + Tomek Links\n",
    "mean_accuracy_smote_tomek = np.mean(accuracy_list_smote_tomek)\n",
    "std_accuracy_smote_tomek = np.std(accuracy_list_smote_tomek)\n",
    "mean_precision_smote_tomek = np.mean(precision_list_smote_tomek)\n",
    "std_precision_smote_tomek = np.std(precision_list_smote_tomek)\n",
    "mean_recall_smote_tomek = np.mean(recall_list_smote_tomek)\n",
    "std_recall_smote_tomek = np.std(recall_list_smote_tomek)\n",
    "mean_f1_smote_tomek = np.mean(f1_list_smote_tomek)\n",
    "std_f1_smote_tomek = np.std(f1_list_smote_tomek)\n",
    "mean_entropy_smote_tomek = np.mean(entropy_list_smote_tomek)\n",
    "std_entropy_smote_tomek = np.std(entropy_list_smote_tomek)\n",
    "\n",
    "# Calculate mean confusion matrix before and after SMOTE + Tomek Links\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_smote_tomek = np.mean(confusion_matrices_smote_tomek, axis=0)\n",
    "\n",
    "# Print results before oversampling\n",
    "print('--- Before Oversampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after SMOTE + Tomek Links\n",
    "print('--- After SMOTE + Tomek Links ---')\n",
    "print('Mean Accuracy:', mean_accuracy_smote_tomek)\n",
    "print('Accuracy Std Dev:', std_accuracy_smote_tomek)\n",
    "print('Mean Precision:', mean_precision_smote_tomek)\n",
    "print('Precision Std Dev:', std_precision_smote_tomek)\n",
    "print('Mean Recall:', mean_recall_smote_tomek)\n",
    "print('Recall Std Dev:', std_recall_smote_tomek)\n",
    "print('Mean F1-score:', mean_f1_smote_tomek)\n",
    "print('F1-score Std Dev:', std_f1_smote_tomek)\n",
    "print('Mean Entropy:', mean_entropy_smote_tomek)\n",
    "print('Entropy Std Dev:', std_entropy_smote_tomek)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_smote_tomek)"
   ],
   "id": "3378c9fe05835af5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After SMOTE + Tomek Links:\n",
      " [[74  7]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 Before Oversampling:\n",
      " [[80  1]\n",
      " [ 4  1]]\n",
      "Confusion Matrix for Fold 2 After SMOTE + Tomek Links:\n",
      " [[77  4]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After SMOTE + Tomek Links:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Oversampling:\n",
      " [[80  0]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 4 After SMOTE + Tomek Links:\n",
      " [[74  6]\n",
      " [ 0  6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 5 Before Oversampling:\n",
      " [[80  0]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After SMOTE + Tomek Links:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 6 After SMOTE + Tomek Links:\n",
      " [[76  4]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After SMOTE + Tomek Links:\n",
      " [[74  6]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 8 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After SMOTE + Tomek Links:\n",
      " [[79  1]\n",
      " [ 1  5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 9 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After SMOTE + Tomek Links:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 Before Oversampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After SMOTE + Tomek Links:\n",
      " [[74  6]\n",
      " [ 0  5]]\n",
      "--- Before Oversampling ---\n",
      "Mean Accuracy: 0.9592065663474691\n",
      "Accuracy Std Dev: 0.011923309742192302\n",
      "Mean Precision: 0.9584241012884636\n",
      "Precision Std Dev: 0.015727743699434752\n",
      "Mean Recall: 0.9592065663474691\n",
      "Recall Std Dev: 0.011923309742192302\n",
      "Mean F1-score: 0.9573048865385758\n",
      "F1-score Std Dev: 0.01386161073251105\n",
      "Mean Entropy: 0.11892311688116444\n",
      "Entropy Std Dev: 0.014337727793459208\n",
      "Mean Confusion Matrix:\n",
      " [[78.9  1.4]\n",
      " [ 2.1  3.4]]\n",
      "--- After SMOTE + Tomek Links ---\n",
      "Mean Accuracy: 0.9394117647058824\n",
      "Accuracy Std Dev: 0.019992817235501417\n",
      "Mean Precision: 0.5380339105339106\n",
      "Precision Std Dev: 0.12932406044515093\n",
      "Mean Recall: 0.8366666666666667\n",
      "Recall Std Dev: 0.12948616399703355\n",
      "Mean F1-score: 0.6436845017727371\n",
      "F1-score Std Dev: 0.09721607000608254\n",
      "Mean Entropy: 0.39815135160123083\n",
      "Entropy Std Dev: 0.02236443014496605\n",
      "Mean Confusion Matrix:\n",
      " [[76.   4.3]\n",
      " [ 0.9  4.6]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amin/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "threshold Moving\n",
   "id": "797c6598c484de40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:26:01.770873Z",
     "start_time": "2024-11-07T20:25:58.281562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Function to find the best threshold for F1-score\n",
    "def find_best_threshold(y_true, y_prob, thresholds):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:, 1] >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "best_thresholds = []\n",
    "f1_list = []\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "roc_auc_list = []\n",
    "entropy_list = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the Logistic Regression model\n",
    "    lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calibrate the model\n",
    "    calibrated_model = CalibratedClassifierCV(lr_model, method='isotonic', cv=5)\n",
    "    calibrated_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities on training set\n",
    "    y_prob_train = calibrated_model.predict_proba(X_train)\n",
    "    \n",
    "    # Define a range of thresholds to test\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    \n",
    "    # Find the best threshold based on training set\n",
    "    best_threshold, _ = find_best_threshold(y_train, y_prob_train, thresholds)\n",
    "    best_thresholds.append(best_threshold)\n",
    "    \n",
    "    # Get predicted probabilities on test set\n",
    "    y_prob_test = calibrated_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = calculate_entropy(y_prob_test)\n",
    "    entropy_list.append(entropy)\n",
    "    \n",
    "    # Make predictions using the best threshold\n",
    "    y_pred = (y_prob_test[:, 1] >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate and store ROC AUC\n",
    "    roc_auc = roc_auc_score(y_test, y_prob_test[:, 1])\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    \n",
    "    # Store the metrics\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(f'Best Threshold for Fold {len(best_thresholds)}: {best_threshold}')\n",
    "    print(f'Confusion Matrix for Fold {len(best_thresholds)}:\\n', confusion_matrices[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "std_accuracy = np.std(accuracy_list)\n",
    "mean_precision = np.mean(precision_list)\n",
    "std_precision = np.std(precision_list)\n",
    "mean_recall = np.mean(recall_list)\n",
    "std_recall = np.std(recall_list)\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "mean_roc_auc = np.mean(roc_auc_list)\n",
    "std_roc_auc = np.std(roc_auc_list)\n",
    "mean_entropy = np.mean(entropy_list)\n",
    "std_entropy = np.std(entropy_list)\n",
    "\n",
    "# Calculate mean confusion matrix\n",
    "mean_conf_matrix = np.mean(confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "# Print the results\n",
    "print('--- Overall Results ---')\n",
    "print('Mean Accuracy:', mean_accuracy)\n",
    "print('Accuracy Std Dev:', std_accuracy)\n",
    "print('Mean Precision:', mean_precision)\n",
    "print('Precision Std Dev:', std_precision)\n",
    "print('Mean Recall:', mean_recall)\n",
    "print('Recall Std Dev:', std_recall)\n",
    "print('Mean F1-score:', mean_f1)\n",
    "print('F1-score Std Dev:', std_f1)\n",
    "print('Mean ROC AUC:', mean_roc_auc)\n",
    "print('ROC AUC Std Dev:', std_roc_auc)\n",
    "print('Mean Entropy:', mean_entropy)\n",
    "print('Entropy Std Dev:', std_entropy)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix)"
   ],
   "id": "7417e4484ed5a421",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for Fold 1: 0.30999999999999994\n",
      "Confusion Matrix for Fold 1:\n",
      " [[79  2]\n",
      " [ 1  4]]\n",
      "Best Threshold for Fold 2: 0.46999999999999986\n",
      "Confusion Matrix for Fold 2:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Best Threshold for Fold 3: 0.45999999999999985\n",
      "Confusion Matrix for Fold 3:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Best Threshold for Fold 4: 0.43999999999999984\n",
      "Confusion Matrix for Fold 4:\n",
      " [[78  2]\n",
      " [ 3  3]]\n",
      "Best Threshold for Fold 5: 0.45999999999999985\n",
      "Confusion Matrix for Fold 5:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 6: 0.3899999999999999\n",
      "Confusion Matrix for Fold 6:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 7: 0.3799999999999999\n",
      "Confusion Matrix for Fold 7:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 8: 0.4099999999999998\n",
      "Confusion Matrix for Fold 8:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 9: 0.3899999999999999\n",
      "Confusion Matrix for Fold 9:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Best Threshold for Fold 10: 0.1\n",
      "Confusion Matrix for Fold 10:\n",
      " [[76  4]\n",
      " [ 0  5]]\n",
      "--- Overall Results ---\n",
      "Mean Accuracy: 0.9580437756497948\n",
      "Accuracy Std Dev: 0.012947284705326113\n",
      "Mean Precision: 0.649484126984127\n",
      "Precision Std Dev: 0.11355726426598035\n",
      "Mean Recall: 0.7433333333333334\n",
      "Recall Std Dev: 0.17387735140993302\n",
      "Mean F1-score: 0.6854367854367854\n",
      "F1-score Std Dev: 0.12377422444411422\n",
      "Mean ROC AUC: 0.9355216049382717\n",
      "ROC AUC Std Dev: 0.0602281071425531\n",
      "Mean Entropy: 0.09424107232984019\n",
      "Entropy Std Dev: 0.017307673002299434\n",
      "Mean Confusion Matrix:\n",
      " [[78  2]\n",
      " [ 1  4]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5c769f15101d23c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
