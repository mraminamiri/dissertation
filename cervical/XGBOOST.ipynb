{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-07T19:57:14.678019Z",
     "start_time": "2024-11-07T19:57:14.675849Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:57:18.322974Z",
     "start_time": "2024-11-07T19:57:18.299582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('cleaned_data.csv')\n",
    "data.head()"
   ],
   "id": "400c94356b08b9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Age  Number of sexual partners  First sexual intercourse  \\\n",
       "0   18                          6                         5   \n",
       "1   15                          0                         4   \n",
       "2   34                          0                         5   \n",
       "3   52                          7                         6   \n",
       "4   46                          5                        11   \n",
       "\n",
       "   Num of pregnancies  Smokes  Smokes (years)  Smokes (packs/year)  \\\n",
       "0                   1       0               0                    0   \n",
       "1                   1       0               0                    0   \n",
       "2                   1       0               0                    0   \n",
       "3                   6       1              23                   49   \n",
       "4                   6       0               0                    0   \n",
       "\n",
       "   Hormonal Contraceptives  Hormonal Contraceptives (years)  IUD  ...  \\\n",
       "0                        0                                0    0  ...   \n",
       "1                        0                                0    0  ...   \n",
       "2                        0                                0    0  ...   \n",
       "3                        1                               29    0  ...   \n",
       "4                        1                               20    0  ...   \n",
       "\n",
       "   STDs: Time since first diagnosis  STDs: Time since last diagnosis  \\\n",
       "0                                 0                                0   \n",
       "1                                 0                                0   \n",
       "2                                 0                                0   \n",
       "3                                 0                                0   \n",
       "4                                 0                                0   \n",
       "\n",
       "   Dx:Cancer  Dx:CIN  Dx:HPV  Dx  Hinselmann  Schiller  Citology  Biopsy  \n",
       "0          0       0       0   0           0         0         0       0  \n",
       "1          0       0       0   0           0         0         0       0  \n",
       "2          0       0       0   0           0         0         0       0  \n",
       "3          1       0       1   0           0         0         0       0  \n",
       "4          0       0       0   0           0         0         0       0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Number of sexual partners</th>\n",
       "      <th>First sexual intercourse</th>\n",
       "      <th>Num of pregnancies</th>\n",
       "      <th>Smokes</th>\n",
       "      <th>Smokes (years)</th>\n",
       "      <th>Smokes (packs/year)</th>\n",
       "      <th>Hormonal Contraceptives</th>\n",
       "      <th>Hormonal Contraceptives (years)</th>\n",
       "      <th>IUD</th>\n",
       "      <th>...</th>\n",
       "      <th>STDs: Time since first diagnosis</th>\n",
       "      <th>STDs: Time since last diagnosis</th>\n",
       "      <th>Dx:Cancer</th>\n",
       "      <th>Dx:CIN</th>\n",
       "      <th>Dx:HPV</th>\n",
       "      <th>Dx</th>\n",
       "      <th>Hinselmann</th>\n",
       "      <th>Schiller</th>\n",
       "      <th>Citology</th>\n",
       "      <th>Biopsy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:57:32.127292Z",
     "start_time": "2024-11-07T19:57:32.122050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X=data.drop(['Biopsy'], axis=1)\n",
    "y=data['Biopsy']"
   ],
   "id": "8a40a5d10863137e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Over Sampling",
   "id": "94d628c5b3935436"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:58:14.948201Z",
     "start_time": "2024-11-07T19:58:10.201171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "import numpy as np\n",
    "import xgboost as xgb  # Import XGBoost\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_smote = []\n",
    "precision_list_smote = []\n",
    "recall_list_smote = []\n",
    "f1_list_smote = []\n",
    "entropy_list_smote = []\n",
    "confusion_matrices_smote = []\n",
    "\n",
    "accuracy_list_adasyn = []\n",
    "precision_list_adasyn = []\n",
    "recall_list_adasyn = []\n",
    "f1_list_adasyn = []\n",
    "entropy_list_adasyn = []\n",
    "confusion_matrices_adasyn = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the XGBoost model without SMOTE/ADASYN\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before SMOTE/ADASYN\n",
    "    y_pred_before = xgb_model.predict(X_test)\n",
    "    y_prob_before = xgb_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before SMOTE/ADASYN\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before SMOTE/ADASYN\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with SMOTE\n",
    "    xgb_model_smote = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model_smote.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Predictions and probabilities after SMOTE\n",
    "    y_pred_smote = xgb_model_smote.predict(X_test)\n",
    "    y_prob_smote = xgb_model_smote.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after SMOTE\n",
    "    entropy_smote = calculate_entropy(y_prob_smote)\n",
    "    entropy_list_smote.append(entropy_smote)\n",
    "    \n",
    "    # Store metrics after SMOTE\n",
    "    accuracy_list_smote.append(accuracy_score(y_test, y_pred_smote))\n",
    "    precision_list_smote.append(precision_score(y_test, y_pred_smote))\n",
    "    recall_list_smote.append(recall_score(y_test, y_pred_smote))\n",
    "    f1_list_smote.append(f1_score(y_test, y_pred_smote))\n",
    "    confusion_matrices_smote.append(confusion_matrix(y_test, y_pred_smote))\n",
    "    \n",
    "    # Apply ADASYN to the training data\n",
    "    adasyn = ADASYN(random_state=42)\n",
    "    X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with ADASYN\n",
    "    xgb_model_adasyn = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model_adasyn.fit(X_train_adasyn, y_train_adasyn)\n",
    "    \n",
    "    # Predictions and probabilities after ADASYN\n",
    "    y_pred_adasyn = xgb_model_adasyn.predict(X_test)\n",
    "    y_prob_adasyn = xgb_model_adasyn.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after ADASYN\n",
    "    entropy_adasyn = calculate_entropy(y_prob_adasyn)\n",
    "    entropy_list_adasyn.append(entropy_adasyn)\n",
    "    \n",
    "    # Store metrics after ADASYN\n",
    "    accuracy_list_adasyn.append(accuracy_score(y_test, y_pred_adasyn))\n",
    "    precision_list_adasyn.append(precision_score(y_test, y_pred_adasyn))\n",
    "    recall_list_adasyn.append(recall_score(y_test, y_pred_adasyn))\n",
    "    f1_list_adasyn.append(f1_score(y_test, y_pred_adasyn))\n",
    "    confusion_matrices_adasyn.append(confusion_matrix(y_test, y_pred_adasyn))\n",
    "    \n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Oversampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_smote)} After SMOTE:\\n', confusion_matrices_smote[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_adasyn)} After ADASYN:\\n', confusion_matrices_adasyn[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before oversampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after SMOTE\n",
    "mean_accuracy_smote = np.mean(accuracy_list_smote)\n",
    "std_accuracy_smote = np.std(accuracy_list_smote)\n",
    "mean_precision_smote = np.mean(precision_list_smote)\n",
    "std_precision_smote = np.std(precision_list_smote)\n",
    "mean_recall_smote = np.mean(recall_list_smote)\n",
    "std_recall_smote = np.std(recall_list_smote)\n",
    "mean_f1_smote = np.mean(f1_list_smote)\n",
    "std_f1_smote = np.std(f1_list_smote)\n",
    "mean_entropy_smote = np.mean(entropy_list_smote)\n",
    "std_entropy_smote = np.std(entropy_list_smote)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after ADASYN\n",
    "mean_accuracy_adasyn = np.mean(accuracy_list_adasyn)\n",
    "std_accuracy_adasyn = np.std(accuracy_list_adasyn)\n",
    "mean_precision_adasyn = np.mean(precision_list_adasyn)\n",
    "std_precision_adasyn = np.std(precision_list_adasyn)\n",
    "mean_recall_adasyn = np.mean(recall_list_adasyn)\n",
    "std_recall_adasyn = np.std(recall_list_adasyn)\n",
    "mean_f1_adasyn = np.mean(f1_list_adasyn)\n",
    "std_f1_adasyn = np.std(f1_list_adasyn)\n",
    "mean_entropy_adasyn = np.mean(entropy_list_adasyn)\n",
    "std_entropy_adasyn = np.std(entropy_list_adasyn)\n",
    "\n",
    "# Calculate mean confusion matrix before and after SMOTE/ADASYN\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_smote = np.mean(confusion_matrices_smote, axis=0)\n",
    "mean_conf_matrix_adasyn = np.mean(confusion_matrices_adasyn, axis=0)\n",
    "\n",
    "# Print results before oversampling\n",
    "print('--- Before Oversampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after SMOTE\n",
    "print('--- After SMOTE ---')\n",
    "print('Mean Accuracy:', mean_accuracy_smote)\n",
    "print('Accuracy Std Dev:', std_accuracy_smote)\n",
    "print('Mean Precision:', mean_precision_smote)\n",
    "print('Precision Std Dev:', std_precision_smote)\n",
    "print('Mean Recall:', mean_recall_smote)\n",
    "print('Recall Std Dev:', std_recall_smote)\n",
    "print('Mean F1-score:', mean_f1_smote)\n",
    "print('F1-score Std Dev:', std_f1_smote)\n",
    "print('Mean Entropy:', mean_entropy_smote)\n",
    "print('Entropy Std Dev:', std_entropy_smote)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_smote)\n",
    "\n",
    "# Print results after ADASYN\n",
    "print('--- After ADASYN ---')\n",
    "print('Mean Accuracy:', mean_accuracy_adasyn)\n",
    "print('Accuracy Std Dev:', std_accuracy_adasyn)\n",
    "print('Mean Precision:', mean_precision_adasyn)\n",
    "print('Precision Std Dev:', std_precision_adasyn)\n",
    "print('Mean Recall:', mean_recall_adasyn)\n",
    "print('Recall Std Dev:', std_recall_adasyn)\n",
    "print('Mean F1-score:', mean_f1_adasyn)\n",
    "print('F1-score Std Dev:', std_f1_adasyn)\n",
    "print('Mean Entropy:', mean_entropy_adasyn)\n",
    "print('Entropy Std Dev:', std_entropy_adasyn)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_adasyn)\n"
   ],
   "id": "73e0c91fec6bb194",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After SMOTE:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 1 After ADASYN:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 2 Before Oversampling:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After SMOTE:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After ADASYN:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 Before Oversampling:\n",
      " [[78  3]\n",
      " [ 4  1]]\n",
      "Confusion Matrix for Fold 3 After SMOTE:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 After ADASYN:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 4 After SMOTE:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 4 After ADASYN:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After SMOTE:\n",
      " [[74  6]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After ADASYN:\n",
      " [[75  5]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 6 After SMOTE:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 After ADASYN:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 5  1]]\n",
      "Confusion Matrix for Fold 7 After SMOTE:\n",
      " [[74  6]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 7 After ADASYN:\n",
      " [[77  3]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 8 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After SMOTE:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After ADASYN:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 9 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 9 After SMOTE:\n",
      " [[79  1]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 9 After ADASYN:\n",
      " [[77  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 10 Before Oversampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After SMOTE:\n",
      " [[75  5]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After ADASYN:\n",
      " [[78  2]\n",
      " [ 0  5]]\n",
      "--- Before Oversampling ---\n",
      "Mean Accuracy: 0.9475786593707249\n",
      "Accuracy Std Dev: 0.018875155875792117\n",
      "Mean Precision: 0.5769047619047619\n",
      "Precision Std Dev: 0.1886911897237523\n",
      "Mean Recall: 0.5399999999999999\n",
      "Recall Std Dev: 0.23560796062763056\n",
      "Mean F1-score: 0.5491452991452992\n",
      "F1-score Std Dev: 0.20498401271114638\n",
      "Mean Entropy: 0.05227316\n",
      "Entropy Std Dev: 0.009688806\n",
      "Mean Confusion Matrix:\n",
      " [[78.3  2. ]\n",
      " [ 2.5  3. ]]\n",
      "--- After SMOTE ---\n",
      "Mean Accuracy: 0.9382489740082078\n",
      "Accuracy Std Dev: 0.018730283152025146\n",
      "Mean Precision: 0.5259523809523811\n",
      "Precision Std Dev: 0.11546121650583283\n",
      "Mean Recall: 0.6833333333333333\n",
      "Recall Std Dev: 0.18514258769331754\n",
      "Mean F1-score: 0.580992340992341\n",
      "F1-score Std Dev: 0.11771501275560821\n",
      "Mean Entropy: 0.096236296\n",
      "Entropy Std Dev: 0.018933669\n",
      "Mean Confusion Matrix:\n",
      " [[76.7  3.6]\n",
      " [ 1.7  3.8]]\n",
      "--- After ADASYN ---\n",
      "Mean Accuracy: 0.9464021887824897\n",
      "Accuracy Std Dev: 0.015807088385332617\n",
      "Mean Precision: 0.5648809523809525\n",
      "Precision Std Dev: 0.11333589682975154\n",
      "Mean Recall: 0.7033333333333334\n",
      "Recall Std Dev: 0.16292465879799917\n",
      "Mean F1-score: 0.6223901098901099\n",
      "F1-score Std Dev: 0.12507505059552976\n",
      "Mean Entropy: 0.0877502\n",
      "Entropy Std Dev: 0.016949525\n",
      "Mean Confusion Matrix:\n",
      " [[77.3  3. ]\n",
      " [ 1.6  3.9]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "UnderSampling",
   "id": "9d18b78ba73ae4b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T19:59:55.214310Z",
     "start_time": "2024-11-07T19:59:48.798469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import NearMiss, ClusterCentroids, TomekLinks, RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_nearmiss = []\n",
    "precision_list_nearmiss = []\n",
    "recall_list_nearmiss = []\n",
    "f1_list_nearmiss = []\n",
    "entropy_list_nearmiss = []\n",
    "confusion_matrices_nearmiss = []\n",
    "\n",
    "accuracy_list_clustercentroids = []\n",
    "precision_list_clustercentroids = []\n",
    "recall_list_clustercentroids = []\n",
    "f1_list_clustercentroids = []\n",
    "entropy_list_clustercentroids = []\n",
    "confusion_matrices_clustercentroids = []\n",
    "\n",
    "accuracy_list_tomeklinks = []\n",
    "precision_list_tomeklinks = []\n",
    "recall_list_tomeklinks = []\n",
    "f1_list_tomeklinks = []\n",
    "entropy_list_tomeklinks = []\n",
    "confusion_matrices_tomeklinks = []\n",
    "\n",
    "accuracy_list_random = []\n",
    "precision_list_random = []\n",
    "recall_list_random = []\n",
    "f1_list_random = []\n",
    "entropy_list_random = []\n",
    "confusion_matrices_random = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the XGBoost model without undersampling\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before undersampling\n",
    "    y_pred_before = xgb_model.predict(X_test)\n",
    "    y_prob_before = xgb_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before undersampling\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before undersampling\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply NearMiss to the training data\n",
    "    nearmiss = NearMiss(version=1)\n",
    "    X_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with NearMiss\n",
    "    xgb_model_nearmiss = XGBClassifier(random_state=42)\n",
    "    xgb_model_nearmiss.fit(X_train_nearmiss, y_train_nearmiss)\n",
    "    \n",
    "    # Predictions and probabilities after NearMiss\n",
    "    y_pred_nearmiss = xgb_model_nearmiss.predict(X_test)\n",
    "    y_prob_nearmiss = xgb_model_nearmiss.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after NearMiss\n",
    "    entropy_nearmiss = calculate_entropy(y_prob_nearmiss)\n",
    "    entropy_list_nearmiss.append(entropy_nearmiss)\n",
    "    \n",
    "    # Store metrics after NearMiss\n",
    "    accuracy_list_nearmiss.append(accuracy_score(y_test, y_pred_nearmiss))\n",
    "    precision_list_nearmiss.append(precision_score(y_test, y_pred_nearmiss))\n",
    "    recall_list_nearmiss.append(recall_score(y_test, y_pred_nearmiss))\n",
    "    f1_list_nearmiss.append(f1_score(y_test, y_pred_nearmiss))\n",
    "    confusion_matrices_nearmiss.append(confusion_matrix(y_test, y_pred_nearmiss))\n",
    "    \n",
    "    # Apply ClusterCentroids to the training data\n",
    "    clustercentroids = ClusterCentroids(random_state=42)\n",
    "    X_train_clustercentroids, y_train_clustercentroids = clustercentroids.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with ClusterCentroids\n",
    "    xgb_model_clustercentroids = XGBClassifier(random_state=42)\n",
    "    xgb_model_clustercentroids.fit(X_train_clustercentroids, y_train_clustercentroids)\n",
    "    \n",
    "    # Predictions and probabilities after ClusterCentroids\n",
    "    y_pred_clustercentroids = xgb_model_clustercentroids.predict(X_test)\n",
    "    y_prob_clustercentroids = xgb_model_clustercentroids.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after ClusterCentroids\n",
    "    entropy_clustercentroids = calculate_entropy(y_prob_clustercentroids)\n",
    "    entropy_list_clustercentroids.append(entropy_clustercentroids)\n",
    "    \n",
    "    # Store metrics after ClusterCentroids\n",
    "    accuracy_list_clustercentroids.append(accuracy_score(y_test, y_pred_clustercentroids))\n",
    "    precision_list_clustercentroids.append(precision_score(y_test, y_pred_clustercentroids))\n",
    "    recall_list_clustercentroids.append(recall_score(y_test, y_pred_clustercentroids))\n",
    "    f1_list_clustercentroids.append(f1_score(y_test, y_pred_clustercentroids))\n",
    "    confusion_matrices_clustercentroids.append(confusion_matrix(y_test, y_pred_clustercentroids))\n",
    "    \n",
    "    # Apply TomekLinks to the training data\n",
    "    tomeklinks = TomekLinks()\n",
    "    X_train_tomeklinks, y_train_tomeklinks = tomeklinks.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with TomekLinks\n",
    "    xgb_model_tomeklinks = XGBClassifier(random_state=42)\n",
    "    xgb_model_tomeklinks.fit(X_train_tomeklinks, y_train_tomeklinks)\n",
    "    \n",
    "    # Predictions and probabilities after TomekLinks\n",
    "    y_pred_tomeklinks = xgb_model_tomeklinks.predict(X_test)\n",
    "    y_prob_tomeklinks = xgb_model_tomeklinks.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after TomekLinks\n",
    "    entropy_tomeklinks = calculate_entropy(y_prob_tomeklinks)\n",
    "    entropy_list_tomeklinks.append(entropy_tomeklinks)\n",
    "    \n",
    "    # Store metrics after TomekLinks\n",
    "    accuracy_list_tomeklinks.append(accuracy_score(y_test, y_pred_tomeklinks))\n",
    "    precision_list_tomeklinks.append(precision_score(y_test, y_pred_tomeklinks))\n",
    "    recall_list_tomeklinks.append(recall_score(y_test, y_pred_tomeklinks))\n",
    "    f1_list_tomeklinks.append(f1_score(y_test, y_pred_tomeklinks))\n",
    "    confusion_matrices_tomeklinks.append(confusion_matrix(y_test, y_pred_tomeklinks))\n",
    "    \n",
    "    # Apply RandomUnderSampler to the training data\n",
    "    random_undersampler = RandomUnderSampler(random_state=42)\n",
    "    X_train_random, y_train_random = random_undersampler.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with RandomUnderSampler\n",
    "    xgb_model_random = XGBClassifier(random_state=42)\n",
    "    xgb_model_random.fit(X_train_random, y_train_random)\n",
    "    \n",
    "    # Predictions and probabilities after RandomUnderSampler\n",
    "    y_pred_random = xgb_model_random.predict(X_test)\n",
    "    y_prob_random = xgb_model_random.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after RandomUnderSampler\n",
    "    entropy_random = calculate_entropy(y_prob_random)\n",
    "    entropy_list_random.append(entropy_random)\n",
    "    \n",
    "    # Store metrics after RandomUnderSampler\n",
    "    accuracy_list_random.append(accuracy_score(y_test, y_pred_random))\n",
    "    precision_list_random.append(precision_score(y_test, y_pred_random))\n",
    "    recall_list_random.append(recall_score(y_test, y_pred_random))\n",
    "    f1_list_random.append(f1_score(y_test, y_pred_random))\n",
    "    confusion_matrices_random.append(confusion_matrix(y_test, y_pred_random))\n",
    "    \n",
    "    # Print confusion matrices for each method\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Undersampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_nearmiss)} After NearMiss:\\n', confusion_matrices_nearmiss[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_clustercentroids)} After ClusterCentroids:\\n', confusion_matrices_clustercentroids[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_tomeklinks)} After TomekLinks:\\n', confusion_matrices_tomeklinks[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_random)} After RandomUnderSampler:\\n', confusion_matrices_random[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before undersampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after NearMiss\n",
    "mean_accuracy_nearmiss = np.mean(accuracy_list_nearmiss)\n",
    "std_accuracy_nearmiss = np.std(accuracy_list_nearmiss)\n",
    "mean_precision_nearmiss = np.mean(precision_list_nearmiss)\n",
    "std_precision_nearmiss = np.std(precision_list_nearmiss)\n",
    "mean_recall_nearmiss = np.mean(recall_list_nearmiss)\n",
    "std_recall_nearmiss = np.std(recall_list_nearmiss)\n",
    "mean_f1_nearmiss = np.mean(f1_list_nearmiss)\n",
    "std_f1_nearmiss = np.std(f1_list_nearmiss)\n",
    "mean_entropy_nearmiss = np.mean(entropy_list_nearmiss)\n",
    "std_entropy_nearmiss = np.std(entropy_list_nearmiss)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after ClusterCentroids\n",
    "mean_accuracy_clustercentroids = np.mean(accuracy_list_clustercentroids)\n",
    "std_accuracy_clustercentroids = np.std(accuracy_list_clustercentroids)\n",
    "mean_precision_clustercentroids = np.mean(precision_list_clustercentroids)\n",
    "std_precision_clustercentroids = np.std(precision_list_clustercentroids)\n",
    "mean_recall_clustercentroids = np.mean(recall_list_clustercentroids)\n",
    "std_recall_clustercentroids = np.std(recall_list_clustercentroids)\n",
    "mean_f1_clustercentroids = np.mean(f1_list_clustercentroids)\n",
    "std_f1_clustercentroids = np.std(f1_list_clustercentroids)\n",
    "mean_entropy_clustercentroids = np.mean(entropy_list_clustercentroids)\n",
    "std_entropy_clustercentroids = np.std(entropy_list_clustercentroids)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after TomekLinks\n",
    "mean_accuracy_tomeklinks = np.mean(accuracy_list_tomeklinks)\n",
    "std_accuracy_tomeklinks = np.std(accuracy_list_tomeklinks)\n",
    "mean_precision_tomeklinks = np.mean(precision_list_tomeklinks)\n",
    "std_precision_tomeklinks = np.std(precision_list_tomeklinks)\n",
    "mean_recall_tomeklinks = np.mean(recall_list_tomeklinks)\n",
    "std_recall_tomeklinks = np.std(recall_list_tomeklinks)\n",
    "mean_f1_tomeklinks = np.mean(f1_list_tomeklinks)\n",
    "std_f1_tomeklinks = np.std(f1_list_tomeklinks)\n",
    "mean_entropy_tomeklinks = np.mean(entropy_list_tomeklinks)\n",
    "std_entropy_tomeklinks = np.std(entropy_list_tomeklinks)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after RandomUnderSampler\n",
    "mean_accuracy_random = np.mean(accuracy_list_random)\n",
    "std_accuracy_random = np.std(accuracy_list_random)\n",
    "mean_precision_random = np.mean(precision_list_random)\n",
    "std_precision_random = np.std(precision_list_random)\n",
    "mean_recall_random = np.mean(recall_list_random)\n",
    "std_recall_random = np.std(recall_list_random)\n",
    "mean_f1_random = np.mean(f1_list_random)\n",
    "std_f1_random = np.std(f1_list_random)\n",
    "mean_entropy_random = np.mean(entropy_list_random)\n",
    "std_entropy_random = np.std(entropy_list_random)\n",
    "\n",
    "# Calculate mean confusion matrix before and after each method\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_nearmiss = np.mean(confusion_matrices_nearmiss, axis=0)\n",
    "mean_conf_matrix_clustercentroids = np.mean(confusion_matrices_clustercentroids, axis=0)\n",
    "mean_conf_matrix_tomeklinks = np.mean(confusion_matrices_tomeklinks, axis=0)\n",
    "mean_conf_matrix_random = np.mean(confusion_matrices_random, axis=0)\n",
    "\n",
    "# Print results before undersampling\n",
    "print('--- Before Undersampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after NearMiss\n",
    "print('--- After NearMiss ---')\n",
    "print('Mean Accuracy:', mean_accuracy_nearmiss)\n",
    "print('Accuracy Std Dev:', std_accuracy_nearmiss)\n",
    "print('Mean Precision:', mean_precision_nearmiss)\n",
    "print('Precision Std Dev:', std_precision_nearmiss)\n",
    "print('Mean Recall:', mean_recall_nearmiss)\n",
    "print('Recall Std Dev:', std_recall_nearmiss)\n",
    "print('Mean F1-score:', mean_f1_nearmiss)\n",
    "print('F1-score Std Dev:', std_f1_nearmiss)\n",
    "print('Mean Entropy:', mean_entropy_nearmiss)\n",
    "print('Entropy Std Dev:', std_entropy_nearmiss)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_nearmiss)\n",
    "\n",
    "# Print results after ClusterCentroids\n",
    "print('--- After ClusterCentroids ---')\n",
    "print('Mean Accuracy:', mean_accuracy_clustercentroids)\n",
    "print('Accuracy Std Dev:', std_accuracy_clustercentroids)\n",
    "print('Mean Precision:', mean_precision_clustercentroids)\n",
    "print('Precision Std Dev:', std_precision_clustercentroids)\n",
    "print('Mean Recall:', mean_recall_clustercentroids)\n",
    "print('Recall Std Dev:', std_recall_clustercentroids)\n",
    "print('Mean F1-score:', mean_f1_clustercentroids)\n",
    "print('F1-score Std Dev:', std_f1_clustercentroids)\n",
    "print('Mean Entropy:', mean_entropy_clustercentroids)\n",
    "print('Entropy Std Dev:', std_entropy_clustercentroids)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_clustercentroids)\n",
    "\n",
    "# Print results after TomekLinks\n",
    "print('--- After TomekLinks ---')\n",
    "print('Mean Accuracy:', mean_accuracy_tomeklinks)\n",
    "print('Accuracy Std Dev:', std_accuracy_tomeklinks)\n",
    "print('Mean Precision:', mean_precision_tomeklinks)\n",
    "print('Precision Std Dev:', std_precision_tomeklinks)\n",
    "print('Mean Recall:', mean_recall_tomeklinks)\n",
    "print('Recall Std Dev:', std_recall_tomeklinks)\n",
    "print('Mean F1-score:', mean_f1_tomeklinks)\n",
    "print('F1-score Std Dev:', std_f1_tomeklinks)\n",
    "print('Mean Entropy:', mean_entropy_tomeklinks)\n",
    "print('Entropy Std Dev:', std_entropy_tomeklinks)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_tomeklinks)\n",
    "\n",
    "# Print results after RandomUnderSampler\n",
    "print('--- After RandomUnderSampler ---')\n",
    "print('Mean Accuracy:', mean_accuracy_random)\n",
    "print('Accuracy Std Dev:', std_accuracy_random)\n",
    "print('Mean Precision:', mean_precision_random)\n",
    "print('Precision Std Dev:', std_precision_random)\n",
    "print('Mean Recall:', mean_recall_random)\n",
    "print('Recall Std Dev:', std_recall_random)\n",
    "print('Mean F1-score:', mean_f1_random)\n",
    "print('F1-score Std Dev:', std_f1_random)\n",
    "print('Mean Entropy:', mean_entropy_random)\n",
    "print('Entropy Std Dev:', std_entropy_random)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_random)\n"
   ],
   "id": "9c65b94f0fdebff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Undersampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After NearMiss:\n",
      " [[35 46]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After ClusterCentroids:\n",
      " [[56 25]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 1 After TomekLinks:\n",
      " [[79  2]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 1 After RandomUnderSampler:\n",
      " [[70 11]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 Before Undersampling:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After NearMiss:\n",
      " [[38 43]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 2 After ClusterCentroids:\n",
      " [[63 18]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 2 After TomekLinks:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After RandomUnderSampler:\n",
      " [[74  7]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 Before Undersampling:\n",
      " [[78  3]\n",
      " [ 4  1]]\n",
      "Confusion Matrix for Fold 3 After NearMiss:\n",
      " [[39 42]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 After ClusterCentroids:\n",
      " [[68 13]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 3 After TomekLinks:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 3 After RandomUnderSampler:\n",
      " [[74  7]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 4 Before Undersampling:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 4 After NearMiss:\n",
      " [[43 37]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 4 After ClusterCentroids:\n",
      " [[57 23]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 4 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 4 After RandomUnderSampler:\n",
      " [[73  7]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 Before Undersampling:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After NearMiss:\n",
      " [[44 36]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 After ClusterCentroids:\n",
      " [[69 11]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 5 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 5 After RandomUnderSampler:\n",
      " [[74  6]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 6 After NearMiss:\n",
      " [[39 41]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6 After ClusterCentroids:\n",
      " [[58 22]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 6 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 6 After RandomUnderSampler:\n",
      " [[74  6]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Undersampling:\n",
      " [[78  2]\n",
      " [ 5  1]]\n",
      "Confusion Matrix for Fold 7 After NearMiss:\n",
      " [[37 43]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 7 After ClusterCentroids:\n",
      " [[65 15]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 7 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 4  2]]\n",
      "Confusion Matrix for Fold 7 After RandomUnderSampler:\n",
      " [[75  5]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After NearMiss:\n",
      " [[40 40]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After ClusterCentroids:\n",
      " [[60 20]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 8 After TomekLinks:\n",
      " [[79  1]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 8 After RandomUnderSampler:\n",
      " [[73  7]\n",
      " [ 0  6]]\n",
      "Confusion Matrix for Fold 9 Before Undersampling:\n",
      " [[79  1]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 9 After NearMiss:\n",
      " [[39 41]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 9 After ClusterCentroids:\n",
      " [[61 19]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 9 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 9 After RandomUnderSampler:\n",
      " [[72  8]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 Before Undersampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After NearMiss:\n",
      " [[31 49]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After ClusterCentroids:\n",
      " [[54 26]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After TomekLinks:\n",
      " [[78  2]\n",
      " [ 0  5]]\n",
      "Confusion Matrix for Fold 10 After RandomUnderSampler:\n",
      " [[67 13]\n",
      " [ 1  4]]\n",
      "--- Before Undersampling ---\n",
      "Mean Accuracy: 0.9475786593707249\n",
      "Accuracy Std Dev: 0.018875155875792117\n",
      "Mean Precision: 0.5769047619047619\n",
      "Precision Std Dev: 0.1886911897237523\n",
      "Mean Recall: 0.5399999999999999\n",
      "Recall Std Dev: 0.23560796062763056\n",
      "Mean F1-score: 0.5491452991452992\n",
      "F1-score Std Dev: 0.20498401271114638\n",
      "Mean Entropy: 0.05227316\n",
      "Entropy Std Dev: 0.009688806\n",
      "Mean Confusion Matrix:\n",
      " [[78.3  2. ]\n",
      " [ 2.5  3. ]]\n",
      "--- After NearMiss ---\n",
      "Mean Accuracy: 0.5115595075239399\n",
      "Accuracy Std Dev: 0.04392064095480977\n",
      "Mean Precision: 0.11533860121052678\n",
      "Precision Std Dev: 0.01887888172870663\n",
      "Mean Recall: 0.9800000000000001\n",
      "Recall Std Dev: 0.05999999999999999\n",
      "Mean F1-score: 0.20599474782231092\n",
      "F1-score Std Dev: 0.030863975198611077\n",
      "Mean Entropy: 0.19206485\n",
      "Entropy Std Dev: 0.022627613\n",
      "Mean Confusion Matrix:\n",
      " [[38.5 41.8]\n",
      " [ 0.1  5.4]]\n",
      "--- After ClusterCentroids ---\n",
      "Mean Accuracy: 0.7738030095759234\n",
      "Accuracy Std Dev: 0.05433783576489502\n",
      "Mean Precision: 0.22440095810098426\n",
      "Precision Std Dev: 0.05498864630818989\n",
      "Mean Recall: 0.96\n",
      "Recall Std Dev: 0.07999999999999999\n",
      "Mean F1-score: 0.3605234203838578\n",
      "F1-score Std Dev: 0.07101888740571867\n",
      "Mean Entropy: 0.36418074\n",
      "Entropy Std Dev: 0.033447567\n",
      "Mean Confusion Matrix:\n",
      " [[61.1 19.2]\n",
      " [ 0.2  5.3]]\n",
      "--- After TomekLinks ---\n",
      "Mean Accuracy: 0.95109439124487\n",
      "Accuracy Std Dev: 0.014436044401033665\n",
      "Mean Precision: 0.6264285714285714\n",
      "Precision Std Dev: 0.12724380690060252\n",
      "Mean Recall: 0.5866666666666667\n",
      "Recall Std Dev: 0.19447936194419763\n",
      "Mean F1-score: 0.5944444444444444\n",
      "F1-score Std Dev: 0.14076564872305464\n",
      "Mean Entropy: 0.05497988\n",
      "Entropy Std Dev: 0.008857109\n",
      "Mean Confusion Matrix:\n",
      " [[78.4  1.9]\n",
      " [ 2.3  3.2]]\n",
      "--- After RandomUnderSampler ---\n",
      "Mean Accuracy: 0.9055129958960328\n",
      "Accuracy Std Dev: 0.029151975174342898\n",
      "Mean Precision: 0.4082759152612094\n",
      "Precision Std Dev: 0.08870562571745938\n",
      "Mean Recall: 0.9233333333333335\n",
      "Recall Std Dev: 0.09433981132056601\n",
      "Mean F1-score: 0.5619324603844726\n",
      "F1-score Std Dev: 0.09821857509975285\n",
      "Mean Entropy: 0.18420716\n",
      "Entropy Std Dev: 0.03632117\n",
      "Mean Confusion Matrix:\n",
      " [[72.6  7.7]\n",
      " [ 0.4  5.1]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ThresholdMoving",
   "id": "2ccf772920d55a8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:00:29.048930Z",
     "start_time": "2024-11-07T20:00:27.503337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Function to find the best threshold for F1-score\n",
    "def find_best_threshold(y_true, y_prob, thresholds):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob[:, 1] >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "best_thresholds = []\n",
    "f1_list = []\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "entropy_list = []\n",
    "confusion_matrices = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the XGBoost model\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get predicted probabilities on training set\n",
    "    y_prob_train = xgb_model.predict_proba(X_train)\n",
    "    \n",
    "    # Define a range of thresholds to test\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    \n",
    "    # Find the best threshold based on training set\n",
    "    best_threshold, _ = find_best_threshold(y_train, y_prob_train, thresholds)\n",
    "    best_thresholds.append(best_threshold)\n",
    "    \n",
    "    # Get predicted probabilities on test set\n",
    "    y_prob_test = xgb_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = calculate_entropy(y_prob_test)\n",
    "    entropy_list.append(entropy)\n",
    "    \n",
    "    # Make predictions using the best threshold\n",
    "    y_pred = (y_prob_test[:, 1] >= best_threshold).astype(int)\n",
    "    \n",
    "    # Store the metrics\n",
    "    f1_list.append(f1_score(y_test, y_pred))\n",
    "    accuracy_list.append(accuracy_score(y_test, y_pred))\n",
    "    precision_list.append(precision_score(y_test, y_pred))\n",
    "    recall_list.append(recall_score(y_test, y_pred))\n",
    "    confusion_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print(f'Best Threshold for Fold {len(best_thresholds)}: {best_threshold}')\n",
    "    print(f'Confusion Matrix for Fold {len(best_thresholds)}:\\n', confusion_matrices[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "std_accuracy = np.std(accuracy_list)\n",
    "mean_precision = np.mean(precision_list)\n",
    "std_precision = np.std(precision_list)\n",
    "mean_recall = np.mean(recall_list)\n",
    "std_recall = np.std(recall_list)\n",
    "mean_f1 = np.mean(f1_list)\n",
    "std_f1 = np.std(f1_list)\n",
    "mean_entropy = np.mean(entropy_list)\n",
    "std_entropy = np.std(entropy_list)\n",
    "\n",
    "# Calculate mean confusion matrix\n",
    "mean_conf_matrix = np.mean(confusion_matrices, axis=0).astype(int)\n",
    "\n",
    "# Print the results\n",
    "print('--- Overall Results ---')\n",
    "print('Mean Accuracy:', mean_accuracy)\n",
    "print('Accuracy Std Dev:', std_accuracy)\n",
    "print('Mean Precision:', mean_precision)\n",
    "print('Precision Std Dev:', std_precision)\n",
    "print('Mean Recall:', mean_recall)\n",
    "print('Recall Std Dev:', std_recall)\n",
    "print('Mean F1-score:', mean_f1)\n",
    "print('F1-score Std Dev:', std_f1)\n",
    "print('Mean Entropy:', mean_entropy)\n",
    "print('Entropy Std Dev:', std_entropy)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix)"
   ],
   "id": "eadfda3b3f909b2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for Fold 1: 0.44999999999999984\n",
      "Confusion Matrix for Fold 1:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Best Threshold for Fold 2: 0.4299999999999998\n",
      "Confusion Matrix for Fold 2:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Best Threshold for Fold 3: 0.2599999999999999\n",
      "Confusion Matrix for Fold 3:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Best Threshold for Fold 4: 0.17999999999999997\n",
      "Confusion Matrix for Fold 4:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 5: 0.22999999999999995\n",
      "Confusion Matrix for Fold 5:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 6: 0.22999999999999995\n",
      "Confusion Matrix for Fold 6:\n",
      " [[77  3]\n",
      " [ 2  4]]\n",
      "Best Threshold for Fold 7: 0.33999999999999986\n",
      "Confusion Matrix for Fold 7:\n",
      " [[78  2]\n",
      " [ 3  3]]\n",
      "Best Threshold for Fold 8: 0.3599999999999999\n",
      "Confusion Matrix for Fold 8:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Best Threshold for Fold 9: 0.3599999999999999\n",
      "Confusion Matrix for Fold 9:\n",
      " [[78  2]\n",
      " [ 2  3]]\n",
      "Best Threshold for Fold 10: 0.3599999999999999\n",
      "Confusion Matrix for Fold 10:\n",
      " [[77  3]\n",
      " [ 0  5]]\n",
      "--- Overall Results ---\n",
      "Mean Accuracy: 0.9510670314637484\n",
      "Accuracy Std Dev: 0.013510033025971506\n",
      "Mean Precision: 0.5969047619047618\n",
      "Precision Std Dev: 0.11378760498234879\n",
      "Mean Recall: 0.6666666666666666\n",
      "Recall Std Dev: 0.19321835661585918\n",
      "Mean F1-score: 0.6236818736818737\n",
      "F1-score Std Dev: 0.1376808245581586\n",
      "Mean Entropy: 0.05227316\n",
      "Entropy Std Dev: 0.009688806\n",
      "Mean Confusion Matrix:\n",
      " [[77  2]\n",
      " [ 1  3]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:01:49.325114Z",
     "start_time": "2024-11-07T20:01:47.235730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# هزینه‌ها\n",
    "cost_false_positive = 1  # هزینه مثبت کاذب\n",
    "cost_false_negative = 100  # هزینه منفی کاذب\n",
    "\n",
    "# تابع برای محاسبه هزینه کلی\n",
    "def calculate_total_cost(y_true, y_pred, cost_fp, cost_fn):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total_cost = (fp * cost_fp) + (fn * cost_fn)\n",
    "    return total_cost\n",
    "\n",
    "# تابع برای پیدا کردن بهترین آستانه\n",
    "def find_best_threshold_cost(y_true, y_prob, thresholds, cost_fp, cost_fn):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    best_cost = float('inf')\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_prob >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        total_cost = calculate_total_cost(y_true, y_pred, cost_fp, cost_fn)\n",
    "        \n",
    "        # می‌خواهیم بهترین F1-اسکور را با کمترین هزینه پیدا کنیم\n",
    "        if f1 > best_f1 or (f1 == best_f1 and total_cost < best_cost):\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            best_cost = total_cost\n",
    "            \n",
    "    return best_threshold, best_f1, best_cost\n",
    "\n",
    "# تعداد تقسیم‌ها\n",
    "n_splits = 10\n",
    "\n",
    "# مقداردهی StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# لیست‌ها برای ذخیره متریک‌ها\n",
    "best_thresholds = []\n",
    "train_f1_list = []\n",
    "test_f1_list = []\n",
    "costs = []\n",
    "roc_auc_list = []\n",
    "recall_list = []\n",
    "precision_list = []\n",
    "accuracy_list = []\n",
    "conf_matrices = []\n",
    "\n",
    "# حلقه برای تقسیم‌های StratifiedKFold\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # مقداردهی و آموزش مدل XGBoost\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # گرفتن احتمال‌های پیش‌بینی شده برای داده‌های آموزشی\n",
    "    y_prob_train = xgb_model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    # تعریف بازه‌ای از آستانه‌ها\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    \n",
    "    # پیدا کردن بهترین آستانه بر اساس داده‌های آموزشی\n",
    "    best_threshold, train_f1, best_cost = find_best_threshold_cost(y_train, y_prob_train, thresholds, cost_false_positive, cost_false_negative)\n",
    "    best_thresholds.append(best_threshold)\n",
    "    costs.append(best_cost)\n",
    "    train_f1_list.append(train_f1)\n",
    "    \n",
    "    # پیش‌بینی با استفاده از بهترین آستانه بر روی داده‌های آزمایشی\n",
    "    y_prob_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob_test >= best_threshold).astype(int)\n",
    "    \n",
    "    # محاسبه متریک‌ها برای داده‌های آزمایشی\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ذخیره متریک‌ها\n",
    "    test_f1_list.append(test_f1)\n",
    "    recall_list.append(recall)\n",
    "    precision_list.append(precision)\n",
    "    accuracy_list.append(accuracy)\n",
    "    roc_auc_list.append(roc_auc)\n",
    "    conf_matrices.append(cm)\n",
    "\n",
    "    # چاپ نتایج برای هر تقسیم\n",
    "    print(f'Fold {len(best_thresholds)} - Best Threshold: {best_threshold}')\n",
    "    print(f'Test F1-Score: {test_f1}, Recall: {recall}, Precision: {precision}, Accuracy: {accuracy}')\n",
    "    print(f'ROC AUC: {roc_auc}, Total Cost: {best_cost}')\n",
    "    print(f'Confusion Matrix:\\n{cm}\\n')\n",
    "\n",
    "# محاسبه میانگین برای هر متریک\n",
    "mean_test_f1 = np.mean(test_f1_list)\n",
    "mean_recall = np.mean(recall_list)\n",
    "mean_precision = np.mean(precision_list)\n",
    "mean_accuracy = np.mean(accuracy_list)\n",
    "mean_roc_auc = np.mean(roc_auc_list)\n",
    "mean_cost = np.mean(costs)\n",
    "\n",
    "# محاسبه میانگین ماتریس‌های آشفتگی\n",
    "mean_conf_matrix = np.mean(conf_matrices, axis=0)\n",
    "\n",
    "# چاپ نتایج کلی\n",
    "print('--- Overall Results ---')\n",
    "print('Mean Test F1-score:', mean_test_f1)\n",
    "print('Mean Recall:', mean_recall)\n",
    "print('Mean Precision:', mean_precision)\n",
    "print('Mean Accuracy:', mean_accuracy)\n",
    "print('Mean ROC AUC:', mean_roc_auc)\n",
    "print('Mean Total Cost:', mean_cost)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix)"
   ],
   "id": "5ab240e62952d85b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Best Threshold: 0.44999999999999984\n",
      "Test F1-Score: 0.4444444444444444, Recall: 0.4, Precision: 0.5, Accuracy: 0.9418604651162791\n",
      "ROC AUC: 0.9580246913580247, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[79  2]\n",
      " [ 3  2]]\n",
      "\n",
      "Fold 2 - Best Threshold: 0.4299999999999998\n",
      "Test F1-Score: 0.4, Recall: 0.4, Precision: 0.4, Accuracy: 0.9302325581395349\n",
      "ROC AUC: 0.888888888888889, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[78  3]\n",
      " [ 3  2]]\n",
      "\n",
      "Fold 3 - Best Threshold: 0.2599999999999999\n",
      "Test F1-Score: 0.5454545454545454, Recall: 0.6, Precision: 0.5, Accuracy: 0.9418604651162791\n",
      "ROC AUC: 0.9160493827160494, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[78  3]\n",
      " [ 2  3]]\n",
      "\n",
      "Fold 4 - Best Threshold: 0.17999999999999997\n",
      "Test F1-Score: 0.7142857142857143, Recall: 0.8333333333333334, Precision: 0.625, Accuracy: 0.9534883720930233\n",
      "ROC AUC: 0.96875, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[77  3]\n",
      " [ 1  5]]\n",
      "\n",
      "Fold 5 - Best Threshold: 0.22999999999999995\n",
      "Test F1-Score: 0.7692307692307693, Recall: 0.8333333333333334, Precision: 0.7142857142857143, Accuracy: 0.9651162790697675\n",
      "ROC AUC: 0.9479166666666666, Total Cost: 1\n",
      "Confusion Matrix:\n",
      "[[78  2]\n",
      " [ 1  5]]\n",
      "\n",
      "Fold 6 - Best Threshold: 0.22999999999999995\n",
      "Test F1-Score: 0.6153846153846154, Recall: 0.6666666666666666, Precision: 0.5714285714285714, Accuracy: 0.9418604651162791\n",
      "ROC AUC: 0.9145833333333334, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[77  3]\n",
      " [ 2  4]]\n",
      "\n",
      "Fold 7 - Best Threshold: 0.33999999999999986\n",
      "Test F1-Score: 0.5454545454545454, Recall: 0.5, Precision: 0.6, Accuracy: 0.9418604651162791\n",
      "ROC AUC: 0.96875, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[78  2]\n",
      " [ 3  3]]\n",
      "\n",
      "Fold 8 - Best Threshold: 0.3599999999999999\n",
      "Test F1-Score: 0.8333333333333334, Recall: 0.8333333333333334, Precision: 0.8333333333333334, Accuracy: 0.9767441860465116\n",
      "ROC AUC: 0.975, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[79  1]\n",
      " [ 1  5]]\n",
      "\n",
      "Fold 9 - Best Threshold: 0.3599999999999999\n",
      "Test F1-Score: 0.6, Recall: 0.6, Precision: 0.6, Accuracy: 0.9529411764705882\n",
      "ROC AUC: 0.9800000000000001, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[78  2]\n",
      " [ 2  3]]\n",
      "\n",
      "Fold 10 - Best Threshold: 0.3599999999999999\n",
      "Test F1-Score: 0.7692307692307693, Recall: 1.0, Precision: 0.625, Accuracy: 0.9647058823529412\n",
      "ROC AUC: 0.9825, Total Cost: 0\n",
      "Confusion Matrix:\n",
      "[[77  3]\n",
      " [ 0  5]]\n",
      "\n",
      "--- Overall Results ---\n",
      "Mean Test F1-score: 0.6236818736818737\n",
      "Mean Recall: 0.6666666666666666\n",
      "Mean Precision: 0.5969047619047618\n",
      "Mean Accuracy: 0.9510670314637484\n",
      "Mean ROC AUC: 0.9500462962962963\n",
      "Mean Total Cost: 0.1\n",
      "Mean Confusion Matrix:\n",
      " [[77.9  2.4]\n",
      " [ 1.8  3.7]]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T20:04:13.929073Z",
     "start_time": "2024-11-07T20:04:10.255314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from imblearn.combine import SMOTETomek  # Import SMOTE + Tomek Links\n",
    "import numpy as np\n",
    "import xgboost as xgb  # Import XGBoost\n",
    "\n",
    "# Define entropy function\n",
    "def calculate_entropy(probabilities):\n",
    "    epsilon = 1e-10  # Small constant to avoid log(0)\n",
    "    return -np.mean(np.sum(probabilities * np.log(probabilities + epsilon), axis=1))\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store metrics for each fold\n",
    "accuracy_list_before = []\n",
    "precision_list_before = []\n",
    "recall_list_before = []\n",
    "f1_list_before = []\n",
    "entropy_list_before = []\n",
    "confusion_matrices_before = []\n",
    "\n",
    "accuracy_list_smote_tomek = []\n",
    "precision_list_smote_tomek = []\n",
    "recall_list_smote_tomek = []\n",
    "f1_list_smote_tomek = []\n",
    "entropy_list_smote_tomek = []\n",
    "confusion_matrices_smote_tomek = []\n",
    "\n",
    "# Loop through the StratifiedKFold splits\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Initialize and train the XGBoost model without SMOTE + Tomek Links\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and probabilities before SMOTE + Tomek Links\n",
    "    y_pred_before = xgb_model.predict(X_test)\n",
    "    y_prob_before = xgb_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy before SMOTE + Tomek Links\n",
    "    entropy_before = calculate_entropy(y_prob_before)\n",
    "    entropy_list_before.append(entropy_before)\n",
    "    \n",
    "    # Store metrics before SMOTE + Tomek Links\n",
    "    accuracy_list_before.append(accuracy_score(y_test, y_pred_before))\n",
    "    precision_list_before.append(precision_score(y_test, y_pred_before))\n",
    "    recall_list_before.append(recall_score(y_test, y_pred_before))\n",
    "    f1_list_before.append(f1_score(y_test, y_pred_before))\n",
    "    confusion_matrices_before.append(confusion_matrix(y_test, y_pred_before))\n",
    "    \n",
    "    # Apply SMOTE + Tomek Links to the training data\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_smote_tomek, y_train_smote_tomek = smote_tomek.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Initialize and train the XGBoost model with SMOTE + Tomek Links\n",
    "    xgb_model_smote_tomek = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    xgb_model_smote_tomek.fit(X_train_smote_tomek, y_train_smote_tomek)\n",
    "    \n",
    "    # Predictions and probabilities after SMOTE + Tomek Links\n",
    "    y_pred_smote_tomek = xgb_model_smote_tomek.predict(X_test)\n",
    "    y_prob_smote_tomek = xgb_model_smote_tomek.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate entropy after SMOTE + Tomek Links\n",
    "    entropy_smote_tomek = calculate_entropy(y_prob_smote_tomek)\n",
    "    entropy_list_smote_tomek.append(entropy_smote_tomek)\n",
    "    \n",
    "    # Store metrics after SMOTE + Tomek Links\n",
    "    accuracy_list_smote_tomek.append(accuracy_score(y_test, y_pred_smote_tomek))\n",
    "    precision_list_smote_tomek.append(precision_score(y_test, y_pred_smote_tomek))\n",
    "    recall_list_smote_tomek.append(recall_score(y_test, y_pred_smote_tomek))\n",
    "    f1_list_smote_tomek.append(f1_score(y_test, y_pred_smote_tomek))\n",
    "    confusion_matrices_smote_tomek.append(confusion_matrix(y_test, y_pred_smote_tomek))\n",
    "    \n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_before)} Before Oversampling:\\n', confusion_matrices_before[-1])\n",
    "    print(f'Confusion Matrix for Fold {len(confusion_matrices_smote_tomek)} After SMOTE + Tomek Links:\\n', confusion_matrices_smote_tomek[-1])\n",
    "\n",
    "# Calculate mean and standard deviation of each metric before oversampling\n",
    "mean_accuracy_before = np.mean(accuracy_list_before)\n",
    "std_accuracy_before = np.std(accuracy_list_before)\n",
    "mean_precision_before = np.mean(precision_list_before)\n",
    "std_precision_before = np.std(precision_list_before)\n",
    "mean_recall_before = np.mean(recall_list_before)\n",
    "std_recall_before = np.std(recall_list_before)\n",
    "mean_f1_before = np.mean(f1_list_before)\n",
    "std_f1_before = np.std(f1_list_before)\n",
    "mean_entropy_before = np.mean(entropy_list_before)\n",
    "std_entropy_before = np.std(entropy_list_before)\n",
    "\n",
    "# Calculate mean and standard deviation of each metric after SMOTE + Tomek Links\n",
    "mean_accuracy_smote_tomek = np.mean(accuracy_list_smote_tomek)\n",
    "std_accuracy_smote_tomek = np.std(accuracy_list_smote_tomek)\n",
    "mean_precision_smote_tomek = np.mean(precision_list_smote_tomek)\n",
    "std_precision_smote_tomek = np.std(precision_list_smote_tomek)\n",
    "mean_recall_smote_tomek = np.mean(recall_list_smote_tomek)\n",
    "std_recall_smote_tomek = np.std(recall_list_smote_tomek)\n",
    "mean_f1_smote_tomek = np.mean(f1_list_smote_tomek)\n",
    "std_f1_smote_tomek = np.std(f1_list_smote_tomek)\n",
    "mean_entropy_smote_tomek = np.mean(entropy_list_smote_tomek)\n",
    "std_entropy_smote_tomek = np.std(entropy_list_smote_tomek)\n",
    "\n",
    "# Calculate mean confusion matrix before and after SMOTE + Tomek Links\n",
    "mean_conf_matrix_before = np.mean(confusion_matrices_before, axis=0)\n",
    "mean_conf_matrix_smote_tomek = np.mean(confusion_matrices_smote_tomek, axis=0)\n",
    "\n",
    "# Print results before oversampling\n",
    "print('--- Before Oversampling ---')\n",
    "print('Mean Accuracy:', mean_accuracy_before)\n",
    "print('Accuracy Std Dev:', std_accuracy_before)\n",
    "print('Mean Precision:', mean_precision_before)\n",
    "print('Precision Std Dev:', std_precision_before)\n",
    "print('Mean Recall:', mean_recall_before)\n",
    "print('Recall Std Dev:', std_recall_before)\n",
    "print('Mean F1-score:', mean_f1_before)\n",
    "print('F1-score Std Dev:', std_f1_before)\n",
    "print('Mean Entropy:', mean_entropy_before)\n",
    "print('Entropy Std Dev:', std_entropy_before)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_before)\n",
    "\n",
    "# Print results after SMOTE + Tomek Links\n",
    "print('--- After SMOTE + Tomek Links ---')\n",
    "print('Mean Accuracy:', mean_accuracy_smote_tomek)\n",
    "print('Accuracy Std Dev:', std_accuracy_smote_tomek)\n",
    "print('Mean Precision:', mean_precision_smote_tomek)\n",
    "print('Precision Std Dev:', std_precision_smote_tomek)\n",
    "print('Mean Recall:', mean_recall_smote_tomek)\n",
    "print('Recall Std Dev:', std_recall_smote_tomek)\n",
    "print('Mean F1-score:', mean_f1_smote_tomek)\n",
    "print('F1-score Std Dev:', std_f1_smote_tomek)\n",
    "print('Mean Entropy:', mean_entropy_smote_tomek)\n",
    "print('Entropy Std Dev:', std_entropy_smote_tomek)\n",
    "print('Mean Confusion Matrix:\\n', mean_conf_matrix_smote_tomek)\n"
   ],
   "id": "12bf867f6fca2e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Fold 1 Before Oversampling:\n",
      " [[79  2]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 1 After SMOTE + Tomek Links:\n",
      " [[77  4]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 2 Before Oversampling:\n",
      " [[78  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 2 After SMOTE + Tomek Links:\n",
      " [[76  5]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 3 Before Oversampling:\n",
      " [[78  3]\n",
      " [ 4  1]]\n",
      "Confusion Matrix for Fold 3 After SMOTE + Tomek Links:\n",
      " [[78  3]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 4 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 4 After SMOTE + Tomek Links:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 5 After SMOTE + Tomek Links:\n",
      " [[75  5]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 6 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 3  3]]\n",
      "Confusion Matrix for Fold 6 After SMOTE + Tomek Links:\n",
      " [[77  3]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 7 Before Oversampling:\n",
      " [[78  2]\n",
      " [ 5  1]]\n",
      "Confusion Matrix for Fold 7 After SMOTE + Tomek Links:\n",
      " [[76  4]\n",
      " [ 2  4]]\n",
      "Confusion Matrix for Fold 8 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 8 After SMOTE + Tomek Links:\n",
      " [[78  2]\n",
      " [ 1  5]]\n",
      "Confusion Matrix for Fold 9 Before Oversampling:\n",
      " [[79  1]\n",
      " [ 2  3]]\n",
      "Confusion Matrix for Fold 9 After SMOTE + Tomek Links:\n",
      " [[77  3]\n",
      " [ 3  2]]\n",
      "Confusion Matrix for Fold 10 Before Oversampling:\n",
      " [[77  3]\n",
      " [ 1  4]]\n",
      "Confusion Matrix for Fold 10 After SMOTE + Tomek Links:\n",
      " [[76  4]\n",
      " [ 0  5]]\n",
      "--- Before Oversampling ---\n",
      "Mean Accuracy: 0.9475786593707249\n",
      "Accuracy Std Dev: 0.018875155875792117\n",
      "Mean Precision: 0.5769047619047619\n",
      "Precision Std Dev: 0.1886911897237523\n",
      "Mean Recall: 0.5399999999999999\n",
      "Recall Std Dev: 0.23560796062763056\n",
      "Mean F1-score: 0.5491452991452992\n",
      "F1-score Std Dev: 0.20498401271114638\n",
      "Mean Entropy: 0.05227316\n",
      "Entropy Std Dev: 0.009688806\n",
      "Mean Confusion Matrix:\n",
      " [[78.3  2. ]\n",
      " [ 2.5  3. ]]\n",
      "--- After SMOTE + Tomek Links ---\n",
      "Mean Accuracy: 0.9405608755129959\n",
      "Accuracy Std Dev: 0.014207638238754969\n",
      "Mean Precision: 0.5223412698412699\n",
      "Precision Std Dev: 0.1029516899012092\n",
      "Mean Recall: 0.7200000000000001\n",
      "Recall Std Dev: 0.16679994670929074\n",
      "Mean F1-score: 0.6015509490509492\n",
      "F1-score Std Dev: 0.11890618279532726\n",
      "Mean Entropy: 0.09553963\n",
      "Entropy Std Dev: 0.021004371\n",
      "Mean Confusion Matrix:\n",
      " [[76.7  3.6]\n",
      " [ 1.5  4. ]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "665da7a88253058b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
